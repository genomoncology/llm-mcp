interactions:
- request:
    body: '{"method":"initialize","params":{"protocolVersion":"2025-03-26","capabilities":{"sampling":{},"roots":{"listChanged":true}},"clientInfo":{"name":"mcp","version":"0.1.0"}},"jsonrpc":"2.0","id":0}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '194'
      content-type:
      - application/json
      host:
      - gitmcp.io
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":0,"result":{"protocolVersion":"2025-03-26","capabilities":{"tools":{"listChanged":true}},"serverInfo":{"name":"GitMCP","version":"1.1.0"}}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 94454943f9e9e995-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 14:36:34 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - a8a8d4bbf983d9afc337e0522f9fed90384352babf92dfd5b6346d3523c0aa3b
    status:
      code: 200
      message: OK
- request:
    body: '{"method":"notifications/initialized","jsonrpc":"2.0"}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '54'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - a8a8d4bbf983d9afc337e0522f9fed90384352babf92dfd5b6346d3523c0aa3b
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: ''
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 94454987dadee995-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 23 May 2025 14:36:44 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 202
      message: Accepted
- request:
    body: '{"method":"tools/list","jsonrpc":"2.0","id":1}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '46'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - a8a8d4bbf983d9afc337e0522f9fed90384352babf92dfd5b6346d3523c0aa3b
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":1,"result":{"tools":[{"name":"fetch_llm_documentation","description":"Fetch
        entire documentation file from GitHub repository: simonw/llm. Useful for general
        questions. Always call this tool first if asked about simonw/llm.","inputSchema":{"type":"object"},"annotations":{"_def":{"options":[{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"typeName":"ZodNull"},"~standard":{"version":1,"vendor":"zod"}}],"typeName":"ZodUnion"},"~standard":{"version":1,"vendor":"zod"}}},{"name":"search_llm_documentation","description":"Semantically
        search within the fetched documentation from GitHub repository: simonw/llm.
        Useful for specific queries.","inputSchema":{"type":"object","properties":{"query":{"type":"string","description":"The
        search query to find relevant documentation"}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},{"name":"search_llm_code","description":"Search
        for code within the GitHub repository: \"simonw/llm\" using the GitHub Search
        API (exact match). Returns matching files for you to query further if relevant.","inputSchema":{"type":"object","properties":{"query":{"type":"string","description":"The
        search query to find relevant code files"},"page":{"type":"number","description":"Page
        number to retrieve (starting from 1). Each page contains 30 results."}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},{"name":"fetch_generic_url_content","description":"Generic
        tool to fetch content from any absolute URL, respecting robots.txt rules.
        Use this to retrieve referenced urls (absolute urls) that were mentioned in
        previously fetched documentation.","inputSchema":{"type":"object","properties":{"url":{"type":"string","description":"The
        URL of the document or page to fetch"}},"required":["url"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}]}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 944549881c30e995-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 14:36:44 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - a8a8d4bbf983d9afc337e0522f9fed90384352babf92dfd5b6346d3523c0aa3b
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - a8a8d4bbf983d9afc337e0522f9fed90384352babf92dfd5b6346d3523c0aa3b
      user-agent:
      - python-httpx/0.28.1
    method: DELETE
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: '{"jsonrpc":"2.0","error":{"code":-32000,"message":"Method not allowed"},"id":null}'
    headers:
      CF-RAY:
      - 944549c70c74e995-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '82'
      Content-Type:
      - text/plain;charset=UTF-8
      Date:
      - Fri, 23 May 2025 14:36:54 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 405
      message: Method Not Allowed
- request:
    body: '{"messages":[{"role":"user","content":"According to the `llm`, what are
      the aliases for the model named ''gpt-3.5-turbo-16k''? Respond **only** with
      JSON formatted as {\"aliases\": [\"alias1\", \"alias2\", ...]}."}],"model":"gpt-4.1","stream":false,"tools":[{"type":"function","function":{"name":"fetch_llm_documentation","description":"Fetch
      entire documentation file from GitHub repository: simonw/llm. Useful for general
      questions. Always call this tool first if asked about simonw/llm.","parameters":{}}},{"type":"function","function":{"name":"search_llm_documentation","description":"Semantically
      search within the fetched documentation from GitHub repository: simonw/llm.
      Useful for specific queries.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant documentation"}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"search_llm_code","description":"Search
      for code within the GitHub repository: \"simonw/llm\" using the GitHub Search
      API (exact match). Returns matching files for you to query further if relevant.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant code files"},"page":{"type":"number","description":"Page
      number to retrieve (starting from 1). Each page contains 30 results."}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"fetch_generic_url_content","description":"Generic
      tool to fetch content from any absolute URL, respecting robots.txt rules. Use
      this to retrieve referenced urls (absolute urls) that were mentioned in previously
      fetched documentation.","parameters":{"type":"object","properties":{"url":{"type":"string","description":"The
      URL of the document or page to fetch"}},"required":["url"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}]}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '2039'
      content-type:
      - application/json
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.79.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.79.0
      x-stainless-read-timeout:
      - '600'
      x-stainless-retry-count:
      - '0'
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.12.7
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA4xT0WrbMBR991eI+xyHOHGS1m9tYQts3aBjjLIUo0jXtmZZciW5NAv592I5iZ02
        g/nBCB2dc889V9oFhIDgkBBgBXWsqmV4S7+p7Vd6t7je3j98ElePq5efvFy+Ir0rVzBqGXrzB5k7
        ssZMV7VEJ7TqYGaQOmxVo2V8NYmiSRR7oNIcZUvLaxfG4yicTqbzcBKHUXxgFlowtJCQ3wEhhOz8
        v/WoOL5CQiaj406F1tIcITkdIgSMlu0OUGuFdVQ5GPUg08qham2rRsoB4LSWKaNS9oW7bzdY90FR
        KdNy+SX++7lY3T/wx5vnX6L8Yb/f3kzEoF4nva29oaxR7BTQAD/tJ++KEQKKVp5rkRpWpFJWKdes
        qVA5ekGLEKAm93DbB+zW8Nyg2a4hWfvAZ+N56Bqz0WG0KAmVglq0a9jDmcw+uLR+GsRlMGsslR9z
        pErpzpoP8umA7E8zkzqvjd7Yd1TIhBK2SA1S66MYTiQ4GvEWoDkbOtRGV7VLnS7RF51F804V+kvZ
        o9OrA+i0o3LAimejC3opR0eFvxWni8goK5D31P5C0oYLPQCCQe8f3VzS7voXKv8f+R5gDGuHPK0N
        csHOO+6PGWzf7L+OnVL2hsGieREMUyfQtPPgmNFGdq8J7NY6rNJMqBxNbYR/UpDVKWNsgex6GnEI
        9sEbAAAA//8DALAORn1cBAAA
    headers:
      CF-RAY:
      - 944549c95f3f000a-ORD
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Fri, 23 May 2025 14:36:55 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=ns.0F2bKODsCkbtv_QQPuc.XgKNF6y3g4Tw6uamvix0-1748011015-1.0.1.1-_N7tnFu94nAOuKp.roWmJtcba.E3SkgDfB_k34mL9j7bByXVQu_gVQRK2AVP1UowE30A_7OrZw1SfPOat.p10wFeCIDcTO.NT_5PtSNriBE;
        path=/; expires=Fri, 23-May-25 15:06:55 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=mlTM2x0hMGVqgfEY6hRp0Yz9t7PMHHdA_Ic5APR.JOc-1748011015579-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      access-control-expose-headers:
      - X-Request-ID
      alt-svc:
      - h3=":443"; ma=86400
      cf-cache-status:
      - DYNAMIC
      openai-organization:
      - genomoncology-2v7ur8
      openai-processing-ms:
      - '665'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=31536000; includeSubDomains; preload
      x-envoy-upstream-service-time:
      - '678'
      x-ratelimit-limit-requests:
      - '5000'
      x-ratelimit-limit-tokens:
      - '800000'
      x-ratelimit-remaining-requests:
      - '4999'
      x-ratelimit-remaining-tokens:
      - '799955'
      x-ratelimit-reset-requests:
      - 12ms
      x-ratelimit-reset-tokens:
      - 3ms
      x-request-id:
      - req_bfa3aa218826a8e37d4e3295a5549198
    status:
      code: 200
      message: OK
- request:
    body: '{"method":"initialize","params":{"protocolVersion":"2025-03-26","capabilities":{"sampling":{},"roots":{"listChanged":true}},"clientInfo":{"name":"mcp","version":"0.1.0"}},"jsonrpc":"2.0","id":0}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '194'
      content-type:
      - application/json
      host:
      - gitmcp.io
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":0,"result":{"protocolVersion":"2025-03-26","capabilities":{"tools":{"listChanged":true}},"serverInfo":{"name":"GitMCP","version":"1.1.0"}}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 944549d00845f571-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 14:36:56 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - ab2288bb5ebc1ccffa780d0ec6a1774f2c3d781184d3c719d2e857da14a9d676
    status:
      code: 200
      message: OK
- request:
    body: '{"method":"notifications/initialized","jsonrpc":"2.0"}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '54'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - ab2288bb5ebc1ccffa780d0ec6a1774f2c3d781184d3c719d2e857da14a9d676
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: ''
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 94454a142c72f571-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 23 May 2025 14:37:06 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 202
      message: Accepted
- request:
    body: '{"method":"tools/call","params":{"name":"search_llm_documentation","arguments":{"query":"gpt-3.5-turbo-16k
      aliases"}},"jsonrpc":"2.0","id":1}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '141'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - ab2288bb5ebc1ccffa780d0ec6a1774f2c3d781184d3c719d2e857da14a9d676
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":1,"result":{"searchQuery":"gpt-3.5-turbo-16k aliases","content":[{"type":"text","text":"##
        Query\n\ngpt-3.5-turbo-16k aliases.\n\n## Response\n\n### Sources:\nImportant:
        you can fetch the full content of any source using the fetch_url_content tool\n\n####
        (simonw/llm/docs/openai-models.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/openai-models.md]
        (Score: 0.57)\n- \n## OpenAI completion models\n\nThe `gpt-3.5-turbo-instruct`
        model is a little different - it is a completion model rather than a chat
        model, described in [the OpenAI completions documentation](https://platform.openai.com/docs/api-reference/completions/create).\n\nCompletion
        models can be called with the `-o logprobs 3` option (not supported by chat
        models) which will cause LLM to store 3 log probabilities for each returned
        token in the SQLite database. Consult [this issue](https://github.com/simonw/llm/issues/284#issuecomment-1724772704)
        for details on how to read these values.\n\n(openai-extra-models)=\n\n\n-
        \n## OpenAI language models\n\nRun `llm models` for a full list of available
        models. The OpenAI models supported by LLM are:\n\n<!-- [[[cog\nfrom click.testing
        import CliRunner\nfrom llm.cli import cli\nresult = CliRunner().invoke(cli,
        [\"models\", \"list\"])\nmodels = [line for line in result.output.split(\"\\n\")
        if line.startswith(\"OpenAI \")]\ncog.out(\"```\\n{}\\n```\".format(\"\\n\".join(models)))\n]]]
        -->\n```\nOpenAI Chat: gpt-4o (aliases: 4o)\nOpenAI Chat: chatgpt-4o-latest
        (aliases: chatgpt-4o)\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\nOpenAI
        Chat: gpt-4o-audio-preview\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\nOpenAI
        Chat: gpt-4o-audio-preview-2024-10-01\nOpenAI Chat: gpt-4o-mini-audio-preview\nOpenAI
        Chat: gpt-4o-mini-audio-preview-2024-12-17\nOpenAI Chat: gpt-4.1 (aliases:
        4.1)\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\nOpenAI Chat: gpt-4.1-nano
        (aliases: 4.1-nano)\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\nOpenAI
        Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\nOpenAI Chat: gpt-4
        (aliases: 4, gpt4)\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\nOpenAI Chat:
        gpt-4-1106-preview\nOpenAI Chat: gpt-4-0125-preview\nOpenAI Chat: gpt-4-turbo-2024-04-09\nOpenAI
        Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\nOpenAI Chat:
        gpt-4.5-preview-2025-02-27\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\nOpenAI
        Chat: o1\nOpenAI Chat: o1-2024-12-17\nOpenAI Chat: o1-preview\nOpenAI Chat:
        o1-mini\nOpenAI Chat: o3-mini\nOpenAI Chat: o3\nOpenAI Chat: o4-mini\nOpenAI
        Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n```\n<!--
        [[[end]]] -->\n\nSee [the OpenAI models documentation](https://platform.openai.com/docs/models)
        for details of each of these.\n\n`gpt-4o-mini` (aliased to `4o-mini`) is the
        least expensive model, and is the default for if you don''t specify a model
        at all. Consult [OpenAI''s model documentation](https://platform.openai.com/docs/models)
        for details of the other models.\n\n\n#### (simonw/llm/llms.txt)[https://raw.githubusercontent.com/simonw/llm/main/llms.txt]
        (Score: 0.55)\n- URLs as input to the model.\n- **System Prompts**: Set a
        system prompt to influence the model''s responses.\n\nKey technical terms
        and topics extracted from the document:\n- LLM command line tool\n- Large
        language models (LLMs)\n- Model options and configuration\n- Attachments and
        multi-modal input\n- System prompts and conversation management\n- Default
        options for models\n- Prompt templates\n- Tool execution and external functions\n-
        Logging and conversation history\n\n\n\n#\n- t.upper()\n\nmodel = llm.get_model(\"gpt-4.1-mini\")\nresponse
        = model.prompt(\"Convert panda to upper\", tools=[upper])\ntool_calls = response.tool_calls()\n```\n\nKey
        technical terms and topics include:\n\n- **LLM**: Large Language Model\n-
        **Model**: A specific LLM instance, e.g., GPT-4o-mini\n- **Prompt**: The input
        text to the model\n- **Response**: The output from the model\n- **Tools**:
        Functions that can be executed by the model\n- **Attachments**: Files or URLs
        that can be passed to the model\n- **Schemas**: JSON schema definitions for
        the response\n- **Conversations**: Ongoing discussions with the model\n\nThe
        API supports various features, including:\n\n* **Lazy Loading**: Responses
        are not evaluated until the `text()` method is called\n* **Async Models**:
        Support for asynchronous models using `llm.get_async_model()`\n* **Streaming
        Responses**: Support for streaming responses as they are generated\n* **Token
        Usage**: Methods for tracking token usage, e.g., `response.usage()` \n\nThe
        document also lists several functions for managing models, aliases, and defaults:\n\n*
        `llm.set_alias(alias, model_id)`: Define a new alias\n* `llm.remove_alias(alias)`:
        Remove an alias\n* `llm.set_default_model(model_id)`: Set the default model\n*
        `llm.get_default_model()`: Get the currently configured default model\n\n\n-
        [related-tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/related-tools.md):
        Related Tools for Working with LLMs\nThis document describes tools designed
        to be used with Large Language Models (LLMs). It covers three main tools:
        `strip-tags`, `ttok`, and `symbex`. `strip-tags` is a command for stripping
        HTML tags, useful for working within token budget limits. `ttok` counts OpenAI
        tokens and can truncate input to a desired number of tokens. `symbex` searches
        for symbols in Python codebases, allowing for code extraction and piping into
        LLMs for various tasks. These tools enhance the functionality and usability
        of LLMs in different applications.\n\nExample usage of `strip-tags` and `ttok`:\n```bash\ncurl
        -s https://www.nytimes.com/ \\\n\n- \nllm --system ''Summarize this'' --model
        gpt-4o --save summarize\n```\n\n**KEY TECHNICAL TERMS:**\n\n1. **Templates**:
        Reusable units that combine prompts, system prompts, models, and schema.\n2.
        **--save**: Option to create a template.\n3. **-t/--template**: Option to
        execute a named template.\n4. **YAML files**: Templates are stored as YAML
        files on disk.\n5. **System prompts**: Can be set using the `system:` key
        in a template.\n6. **Fragments**: Can be referenced in templates using the
        `fragments:` and `system_fragments:` keys.\n7. **Options**: Default options
        can be set using the `options:` key in a template.\n8. **Schemas**: Can be
        embedded in templates using the `schema_object:` key.\n9. **Template loaders**:
        Plugins can register prefixes to load templates from external sources.\n\n\n-
        [tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md):
        Tools in LLM for Executing Functions\nThe document explains how Large Language
        Models (LLMs) can execute tools as part of their responses. A tool is a function
        that the model can request to be executed, with a list of available tools
        provided in the initial prompt. The model can then choose to call one or more
        tools, and the code executes the specified tool with the provided arguments.
        The output of the tool execution is then included in the model''s next prompt.
        LLM implements tools as Python functions, which can take any number of arguments
        and return a string or an object that can be converted to a string. The functions
        can be defined directly in the Python API or via plugins/command-line arguments.
        Tools can be used with the LLM command-line tool or the Python API.\n\nFor
        example, a tool function in Python can be defined as:\n```python\ndef greet(name:
        str) -> str:\n    \"\"\"Return a personalized greeting.\"\"\"\n    return
        f\"Hello, {name}!\"\n```\nThe docstring of the function becomes the description
        that is passed to the model.\n\nThe document also mentions that extra examples
        of using tools can be found for the [command-line tool](usage-tools) and [Python
        API](python-api-tools).\n\n- becomes the description that is passed to the
        model.\n\nThe document also mentions that extra examples of using tools can
        be found for the [command-line tool](usage-tools) and [Python API](python-api-tools).\n\nKEY
        TERMS:\n- Tools\n- Large Language Models\n- Functions\n- Python API\n- Command-line
        interface\n- Plugin hooks\n- Tool execution\n- Model prompts\n- Function definitions\n-
        Docstring \n\nLet me know if I can help with anything else.\n\n\n- [usage.md](https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md):
        LLM Command Line Usage and Options\nThe document provides a comprehensive
        guide to using the LLM command line tool, which allows users to interact with
        large language models (LLMs) from the command line. The tool supports various
        models, including OpenAI''s GPT-4o and GPT-4o-mini, and enables features like
        text generation, conversation continuation, and tool execution. Users can
        run prompts, list available models, and configure default options for specific
        models. The document covers usage examples, model options, attachments, system
        prompts, and conversation management.\n\nTo run a prompt, users can simply
        type `llm ''their prompt''`. The tool also supports executing prompts with
        specific models, such as `llm -m gpt-4o ''Ten names for cheesecakes''`. \n\nExample
        usage of listing models:\n```bash\nllm models\n```\n\nExample usage of setting
        default options for a model:\n```bash\nllm models options set gpt-4o temperature
        0.5\n```\n\nThe document also explains how to use system prompts, which allow
        users to set a specific tone or style for the model''s responses. \n\nSome
        key features of the tool include:\n- **Prompt Execution**: Run prompts with
        specific models, e.g., `llm -m gpt-4o ''Ten names for cheesecakes''`.\n- **Model
        Options**: Configure model-specific options, such as temperature and max tokens.\n-
        **Attachments**: Pass files or URLs as input to the model.\n- **System Prompts**:
        Set a system prompt to influence the model''s responses.\n\nKey technical
        terms and topics extracted from the document:\n- LLM command line tool\n-
        Large languag\n\n#### (simonw/llm/docs/usage.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md]
        (Score: 0.55)\n- \n\nSome models are completion models - rather than being
        tuned to respond to chat style prompts, they are designed to complete a sentence
        or paragraph.\n\nAn example of this is the `gpt-3.5-turbo-instruct` OpenAI
        model.\n\nYou can prompt that model the same way as the chat models, but be
        aware that the prompt format that works best is likely to differ.\n\n```bash\nllm
        -m gpt-3.5-turbo-instruct ''Reasons to tame a wild beaver:''\n```\n\n(usage-chat)=\n\n\n\n####
        (simonw/llm/docs/tools.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md]
        (Score: 0.55)\n- \n\nMany Large Language Models have been trained to execute
        tools as part of responding to a prompt. LLM supports tool usage with both
        the command-line interface and the Python API.\n\n#\n\n#### (simonw/llm/docs/aliases.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/aliases.md]
        (Score: 0.54)\n- \n# Model aliases\n\nLLM supports model aliases, which allow
        you to refer to a model by a short name instead of its full ID.\n\n## Listing
        aliases\n\nTo list current aliases, run this:\n\n```bash\nllm aliases\n```\nExample
        output:\n\n<!-- [[[cog\nfrom click.testing import CliRunner\nfrom llm.cli
        import cli\nresult = CliRunner().invoke(cli, [\"aliases\", \"list\"])\ncog.out(\"```\\n{}```\".format(result.output))\n]]]
        -->\n```\n4o                  : gpt-4o\nchatgpt-4o          : chatgpt-4o-latest\n4o-mini             :
        gpt-4o-mini\n4.1                 : gpt-4.1\n4.1-mini            : gpt-4.1-mini\n4.1-nano            :
        gpt-4.1-nano\n3.5                 : gpt-3.5-turbo\nchatgpt             : gpt-3.5-turbo\nchatgpt-16k         :
        gpt-3.5-turbo-16k\n3.5-16k             : gpt-3.5-turbo-16k\n4                   :
        gpt-4\ngpt4                : gpt-4\n4-32k               : gpt-4-32k\ngpt-4-turbo-preview
        : gpt-4-turbo\n4-turbo             : gpt-4-turbo\n4t                  : gpt-4-turbo\ngpt-4.5             :
        gpt-4.5-preview\n3.5-instruct        : gpt-3.5-turbo-instruct\nchatgpt-instruct    :
        gpt-3.5-turbo-instruct\nada                 : text-embedding-ada-002 (embedding)\nada-002             :
        text-embedding-ada-002 (embedding)\n3-small             : text-embedding-3-small
        (embedding)\n3-large             : text-embedding-3-large (embedding)\n3-small-512         :
        text-embedding-3-small-512 (embedding)\n3-large-256         : text-embedding-3-large-256
        (embedding)\n3-large-1024        : text-embedding-3-large-1024 (embedding)\n```\n<!--
        [[[end]]] -->\n\nAdd `--json` to get that list back as JSON:\n\n```bash\nllm
        aliases list --json\n```\nExample output:\n```json\n{\n    \"3.5\": \"gpt-3.5-turbo\",\n    \"chatgpt\":
        \"gpt-3.5-turbo\",\n    \"4\": \"gpt-4\",\n    \"gpt4\": \"gpt-4\",\n    \"ada\":
        \"ada-002\"\n}\n```\n\n\n\n#### (simonw/llm/docs/schemas.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/schemas.md]
        (Score: 0.53)\n- \n\nLarge Language Models are very good at producing structured
        output as JSON or other formats. LLM''s **schemas** feature allows you to
        define the exact structure of JSON data you want to receive from a model.\n\nThis
        feature is supported by models from OpenAI, Anthropic, Google Gemini and can
        be implemented for others {ref}`via plugins <advanced-model-plugins-schemas>`.\n\nThis
        page describes schemas used via the `llm` command-line tool. Schemas can also
        be used from the {ref}`Python API <python-api-schemas>`.\n\n(schemas-tutorial)=\n\n#\n"}]}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 94454a147d21f571-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 14:37:06 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - ab2288bb5ebc1ccffa780d0ec6a1774f2c3d781184d3c719d2e857da14a9d676
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - ab2288bb5ebc1ccffa780d0ec6a1774f2c3d781184d3c719d2e857da14a9d676
      user-agent:
      - python-httpx/0.28.1
    method: DELETE
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: '{"jsonrpc":"2.0","error":{"code":-32000,"message":"Method not allowed"},"id":null}'
    headers:
      CF-RAY:
      - 94454a751e02f571-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '82'
      Content-Type:
      - text/plain;charset=UTF-8
      Date:
      - Fri, 23 May 2025 14:37:22 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 405
      message: Method Not Allowed
- request:
    body: '{"messages":[{"role":"user","content":"According to the `llm`, what are
      the aliases for the model named ''gpt-3.5-turbo-16k''? Respond **only** with
      JSON formatted as {\"aliases\": [\"alias1\", \"alias2\", ...]}."},{"role":"assistant","tool_calls":[{"type":"function","id":"call_k7K4zGhHMRdYAqWikSsOBA0i","function":{"name":"search_llm_documentation","arguments":"{\"query\":
      \"gpt-3.5-turbo-16k aliases\"}"}}]},{"role":"tool","tool_call_id":"call_k7K4zGhHMRdYAqWikSsOBA0i","content":"##
      Query\n\ngpt-3.5-turbo-16k aliases.\n\n## Response\n\n### Sources:\nImportant:
      you can fetch the full content of any source using the fetch_url_content tool\n\n####
      (simonw/llm/docs/openai-models.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/openai-models.md]
      (Score: 0.57)\n- \n## OpenAI completion models\n\nThe `gpt-3.5-turbo-instruct`
      model is a little different - it is a completion model rather than a chat model,
      described in [the OpenAI completions documentation](https://platform.openai.com/docs/api-reference/completions/create).\n\nCompletion
      models can be called with the `-o logprobs 3` option (not supported by chat
      models) which will cause LLM to store 3 log probabilities for each returned
      token in the SQLite database. Consult [this issue](https://github.com/simonw/llm/issues/284#issuecomment-1724772704)
      for details on how to read these values.\n\n(openai-extra-models)=\n\n\n- \n##
      OpenAI language models\n\nRun `llm models` for a full list of available models.
      The OpenAI models supported by LLM are:\n\n<!-- [[[cog\nfrom click.testing import
      CliRunner\nfrom llm.cli import cli\nresult = CliRunner().invoke(cli, [\"models\",
      \"list\"])\nmodels = [line for line in result.output.split(\"\\n\") if line.startswith(\"OpenAI
      \")]\ncog.out(\"```\\n{}\\n```\".format(\"\\n\".join(models)))\n]]] -->\n```\nOpenAI
      Chat: gpt-4o (aliases: 4o)\nOpenAI Chat: chatgpt-4o-latest (aliases: chatgpt-4o)\nOpenAI
      Chat: gpt-4o-mini (aliases: 4o-mini)\nOpenAI Chat: gpt-4o-audio-preview\nOpenAI
      Chat: gpt-4o-audio-preview-2024-12-17\nOpenAI Chat: gpt-4o-audio-preview-2024-10-01\nOpenAI
      Chat: gpt-4o-mini-audio-preview\nOpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\nOpenAI
      Chat: gpt-4.1 (aliases: 4.1)\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\nOpenAI
      Chat: gpt-4.1-nano (aliases: 4.1-nano)\nOpenAI Chat: gpt-3.5-turbo (aliases:
      3.5, chatgpt)\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\nOpenAI
      Chat: gpt-4 (aliases: 4, gpt4)\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\nOpenAI
      Chat: gpt-4-1106-preview\nOpenAI Chat: gpt-4-0125-preview\nOpenAI Chat: gpt-4-turbo-2024-04-09\nOpenAI
      Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\nOpenAI Chat:
      gpt-4.5-preview-2025-02-27\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\nOpenAI
      Chat: o1\nOpenAI Chat: o1-2024-12-17\nOpenAI Chat: o1-preview\nOpenAI Chat:
      o1-mini\nOpenAI Chat: o3-mini\nOpenAI Chat: o3\nOpenAI Chat: o4-mini\nOpenAI
      Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n```\n<!--
      [[[end]]] -->\n\nSee [the OpenAI models documentation](https://platform.openai.com/docs/models)
      for details of each of these.\n\n`gpt-4o-mini` (aliased to `4o-mini`) is the
      least expensive model, and is the default for if you don''t specify a model
      at all. Consult [OpenAI''s model documentation](https://platform.openai.com/docs/models)
      for details of the other models.\n\n\n#### (simonw/llm/llms.txt)[https://raw.githubusercontent.com/simonw/llm/main/llms.txt]
      (Score: 0.55)\n- URLs as input to the model.\n- **System Prompts**: Set a system
      prompt to influence the model''s responses.\n\nKey technical terms and topics
      extracted from the document:\n- LLM command line tool\n- Large language models
      (LLMs)\n- Model options and configuration\n- Attachments and multi-modal input\n-
      System prompts and conversation management\n- Default options for models\n-
      Prompt templates\n- Tool execution and external functions\n- Logging and conversation
      history\n\n\n\n#\n- t.upper()\n\nmodel = llm.get_model(\"gpt-4.1-mini\")\nresponse
      = model.prompt(\"Convert panda to upper\", tools=[upper])\ntool_calls = response.tool_calls()\n```\n\nKey
      technical terms and topics include:\n\n- **LLM**: Large Language Model\n- **Model**:
      A specific LLM instance, e.g., GPT-4o-mini\n- **Prompt**: The input text to
      the model\n- **Response**: The output from the model\n- **Tools**: Functions
      that can be executed by the model\n- **Attachments**: Files or URLs that can
      be passed to the model\n- **Schemas**: JSON schema definitions for the response\n-
      **Conversations**: Ongoing discussions with the model\n\nThe API supports various
      features, including:\n\n* **Lazy Loading**: Responses are not evaluated until
      the `text()` method is called\n* **Async Models**: Support for asynchronous
      models using `llm.get_async_model()`\n* **Streaming Responses**: Support for
      streaming responses as they are generated\n* **Token Usage**: Methods for tracking
      token usage, e.g., `response.usage()` \n\nThe document also lists several functions
      for managing models, aliases, and defaults:\n\n* `llm.set_alias(alias, model_id)`:
      Define a new alias\n* `llm.remove_alias(alias)`: Remove an alias\n* `llm.set_default_model(model_id)`:
      Set the default model\n* `llm.get_default_model()`: Get the currently configured
      default model\n\n\n- [related-tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/related-tools.md):
      Related Tools for Working with LLMs\nThis document describes tools designed
      to be used with Large Language Models (LLMs). It covers three main tools: `strip-tags`,
      `ttok`, and `symbex`. `strip-tags` is a command for stripping HTML tags, useful
      for working within token budget limits. `ttok` counts OpenAI tokens and can
      truncate input to a desired number of tokens. `symbex` searches for symbols
      in Python codebases, allowing for code extraction and piping into LLMs for various
      tasks. These tools enhance the functionality and usability of LLMs in different
      applications.\n\nExample usage of `strip-tags` and `ttok`:\n```bash\ncurl -s
      https://www.nytimes.com/ \\\n\n- \nllm --system ''Summarize this'' --model gpt-4o
      --save summarize\n```\n\n**KEY TECHNICAL TERMS:**\n\n1. **Templates**: Reusable
      units that combine prompts, system prompts, models, and schema.\n2. **--save**:
      Option to create a template.\n3. **-t/--template**: Option to execute a named
      template.\n4. **YAML files**: Templates are stored as YAML files on disk.\n5.
      **System prompts**: Can be set using the `system:` key in a template.\n6. **Fragments**:
      Can be referenced in templates using the `fragments:` and `system_fragments:`
      keys.\n7. **Options**: Default options can be set using the `options:` key in
      a template.\n8. **Schemas**: Can be embedded in templates using the `schema_object:`
      key.\n9. **Template loaders**: Plugins can register prefixes to load templates
      from external sources.\n\n\n- [tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md):
      Tools in LLM for Executing Functions\nThe document explains how Large Language
      Models (LLMs) can execute tools as part of their responses. A tool is a function
      that the model can request to be executed, with a list of available tools provided
      in the initial prompt. The model can then choose to call one or more tools,
      and the code executes the specified tool with the provided arguments. The output
      of the tool execution is then included in the model''s next prompt. LLM implements
      tools as Python functions, which can take any number of arguments and return
      a string or an object that can be converted to a string. The functions can be
      defined directly in the Python API or via plugins/command-line arguments. Tools
      can be used with the LLM command-line tool or the Python API.\n\nFor example,
      a tool function in Python can be defined as:\n```python\ndef greet(name: str)
      -> str:\n    \"\"\"Return a personalized greeting.\"\"\"\n    return f\"Hello,
      {name}!\"\n```\nThe docstring of the function becomes the description that is
      passed to the model.\n\nThe document also mentions that extra examples of using
      tools can be found for the [command-line tool](usage-tools) and [Python API](python-api-tools).\n\n-
      becomes the description that is passed to the model.\n\nThe document also mentions
      that extra examples of using tools can be found for the [command-line tool](usage-tools)
      and [Python API](python-api-tools).\n\nKEY TERMS:\n- Tools\n- Large Language
      Models\n- Functions\n- Python API\n- Command-line interface\n- Plugin hooks\n-
      Tool execution\n- Model prompts\n- Function definitions\n- Docstring \n\nLet
      me know if I can help with anything else.\n\n\n- [usage.md](https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md):
      LLM Command Line Usage and Options\nThe document provides a comprehensive guide
      to using the LLM command line tool, which allows users to interact with large
      language models (LLMs) from the command line. The tool supports various models,
      including OpenAI''s GPT-4o and GPT-4o-mini, and enables features like text generation,
      conversation continuation, and tool execution. Users can run prompts, list available
      models, and configure default options for specific models. The document covers
      usage examples, model options, attachments, system prompts, and conversation
      management.\n\nTo run a prompt, users can simply type `llm ''their prompt''`.
      The tool also supports executing prompts with specific models, such as `llm
      -m gpt-4o ''Ten names for cheesecakes''`. \n\nExample usage of listing models:\n```bash\nllm
      models\n```\n\nExample usage of setting default options for a model:\n```bash\nllm
      models options set gpt-4o temperature 0.5\n```\n\nThe document also explains
      how to use system prompts, which allow users to set a specific tone or style
      for the model''s responses. \n\nSome key features of the tool include:\n- **Prompt
      Execution**: Run prompts with specific models, e.g., `llm -m gpt-4o ''Ten names
      for cheesecakes''`.\n- **Model Options**: Configure model-specific options,
      such as temperature and max tokens.\n- **Attachments**: Pass files or URLs as
      input to the model.\n- **System Prompts**: Set a system prompt to influence
      the model''s responses.\n\nKey technical terms and topics extracted from the
      document:\n- LLM command line tool\n- Large languag\n\n#### (simonw/llm/docs/usage.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md]
      (Score: 0.55)\n- \n\nSome models are completion models - rather than being tuned
      to respond to chat style prompts, they are designed to complete a sentence or
      paragraph.\n\nAn example of this is the `gpt-3.5-turbo-instruct` OpenAI model.\n\nYou
      can prompt that model the same way as the chat models, but be aware that the
      prompt format that works best is likely to differ.\n\n```bash\nllm -m gpt-3.5-turbo-instruct
      ''Reasons to tame a wild beaver:''\n```\n\n(usage-chat)=\n\n\n\n#### (simonw/llm/docs/tools.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md]
      (Score: 0.55)\n- \n\nMany Large Language Models have been trained to execute
      tools as part of responding to a prompt. LLM supports tool usage with both the
      command-line interface and the Python API.\n\n#\n\n#### (simonw/llm/docs/aliases.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/aliases.md]
      (Score: 0.54)\n- \n# Model aliases\n\nLLM supports model aliases, which allow
      you to refer to a model by a short name instead of its full ID.\n\n## Listing
      aliases\n\nTo list current aliases, run this:\n\n```bash\nllm aliases\n```\nExample
      output:\n\n<!-- [[[cog\nfrom click.testing import CliRunner\nfrom llm.cli import
      cli\nresult = CliRunner().invoke(cli, [\"aliases\", \"list\"])\ncog.out(\"```\\n{}```\".format(result.output))\n]]]
      -->\n```\n4o                  : gpt-4o\nchatgpt-4o          : chatgpt-4o-latest\n4o-mini             :
      gpt-4o-mini\n4.1                 : gpt-4.1\n4.1-mini            : gpt-4.1-mini\n4.1-nano            :
      gpt-4.1-nano\n3.5                 : gpt-3.5-turbo\nchatgpt             : gpt-3.5-turbo\nchatgpt-16k         :
      gpt-3.5-turbo-16k\n3.5-16k             : gpt-3.5-turbo-16k\n4                   :
      gpt-4\ngpt4                : gpt-4\n4-32k               : gpt-4-32k\ngpt-4-turbo-preview
      : gpt-4-turbo\n4-turbo             : gpt-4-turbo\n4t                  : gpt-4-turbo\ngpt-4.5             :
      gpt-4.5-preview\n3.5-instruct        : gpt-3.5-turbo-instruct\nchatgpt-instruct    :
      gpt-3.5-turbo-instruct\nada                 : text-embedding-ada-002 (embedding)\nada-002             :
      text-embedding-ada-002 (embedding)\n3-small             : text-embedding-3-small
      (embedding)\n3-large             : text-embedding-3-large (embedding)\n3-small-512         :
      text-embedding-3-small-512 (embedding)\n3-large-256         : text-embedding-3-large-256
      (embedding)\n3-large-1024        : text-embedding-3-large-1024 (embedding)\n```\n<!--
      [[[end]]] -->\n\nAdd `--json` to get that list back as JSON:\n\n```bash\nllm
      aliases list --json\n```\nExample output:\n```json\n{\n    \"3.5\": \"gpt-3.5-turbo\",\n    \"chatgpt\":
      \"gpt-3.5-turbo\",\n    \"4\": \"gpt-4\",\n    \"gpt4\": \"gpt-4\",\n    \"ada\":
      \"ada-002\"\n}\n```\n\n\n\n#### (simonw/llm/docs/schemas.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/schemas.md]
      (Score: 0.53)\n- \n\nLarge Language Models are very good at producing structured
      output as JSON or other formats. LLM''s **schemas** feature allows you to define
      the exact structure of JSON data you want to receive from a model.\n\nThis feature
      is supported by models from OpenAI, Anthropic, Google Gemini and can be implemented
      for others {ref}`via plugins <advanced-model-plugins-schemas>`.\n\nThis page
      describes schemas used via the `llm` command-line tool. Schemas can also be
      used from the {ref}`Python API <python-api-schemas>`.\n\n(schemas-tutorial)=\n\n#\n"}],"model":"gpt-4.1","stream":false,"tools":[{"type":"function","function":{"name":"fetch_llm_documentation","description":"Fetch
      entire documentation file from GitHub repository: simonw/llm. Useful for general
      questions. Always call this tool first if asked about simonw/llm.","parameters":{}}},{"type":"function","function":{"name":"search_llm_documentation","description":"Semantically
      search within the fetched documentation from GitHub repository: simonw/llm.
      Useful for specific queries.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant documentation"}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"search_llm_code","description":"Search
      for code within the GitHub repository: \"simonw/llm\" using the GitHub Search
      API (exact match). Returns matching files for you to query further if relevant.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant code files"},"page":{"type":"number","description":"Page
      number to retrieve (starting from 1). Each page contains 30 results."}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"fetch_generic_url_content","description":"Generic
      tool to fetch content from any absolute URL, respecting robots.txt rules. Use
      this to retrieve referenced urls (absolute urls) that were mentioned in previously
      fetched documentation.","parameters":{"type":"object","properties":{"url":{"type":"string","description":"The
      URL of the document or page to fetch"}},"required":["url"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}]}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '15587'
      content-type:
      - application/json
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.79.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.79.0
      x-stainless-read-timeout:
      - '600'
      x-stainless-retry-count:
      - '0'
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.12.7
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAAwAAAP//jFJdi9swEHz3rxD7HAfb56SXPJa+9o7CUSjnYBRpbasnS0Jal+uF/Pci
        Oxc7/YC+CKTZGc3s7ilhDJSEPQPRcRK90+lH/mC/iPzzY/bqBG2fnt5696nry91x8/UbrCLDHr+j
        oHfWWtjeaSRlzQQLj5wwquYfyvssz7OyGIHeStSR1jpKy3WeFlmxSbMyzcsLs7NKYIA9e04YY+w0
        ntGjkfgKe5at3l96DIG3CPtrEWPgrY4vwENQgbghWM2gsIbQjLZPFXCteMBQxb+qMUc0lW9fKlix
        Cu7Wm+lyOC81PDZD4DGDGbReANwYSzz2YHR/uCDnq19tW+ftMfxGhUYZFbraIw/WRG+BrIMRPSeM
        Hca+DDdRwXnbO6rJvuD43d39djcJwjyLGS7yC0iWuF7Sdpd+3irWEokrHRa9BcFFh3LmzoPgg1R2
        ASSL3H/a+Zv2lF2Z9n/kZ0AIdISydh6lEreR5zKPcVf/VXbt82gYAvofSmBNCn2chcSGD3raIgg/
        A2FfN8q06J1X0yo1rhZCbFHsilxCck5+AQAA//8DADz+ZBhUAwAA
    headers:
      CF-RAY:
      - 94454a75a8e4acaa-ORD
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Fri, 23 May 2025 14:37:22 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=kNy7Yoy.gOoVI9loB7JNTApt4iLK7o_Lyy1itdfzFZI-1748011042-1.0.1.1-oV8MsnRoldEq.8pNjtBDd0JdMC_UOa1V1YKX_si03LmQml13aj_5MWnXJsvcxSkj7UlhyOZkf2UAQCLGQ9pqD3SIs2PvPmIMtCiNWUryHSY;
        path=/; expires=Fri, 23-May-25 15:07:22 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=pPWwZOHPWtnC7AiqFvF_kqjcjQpza95n003TLAZ8c40-1748011042977-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      access-control-expose-headers:
      - X-Request-ID
      alt-svc:
      - h3=":443"; ma=86400
      cf-cache-status:
      - DYNAMIC
      openai-organization:
      - genomoncology-2v7ur8
      openai-processing-ms:
      - '685'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=31536000; includeSubDomains; preload
      x-envoy-upstream-service-time:
      - '689'
      x-ratelimit-limit-requests:
      - '5000'
      x-ratelimit-limit-tokens:
      - '800000'
      x-ratelimit-remaining-requests:
      - '4999'
      x-ratelimit-remaining-tokens:
      - '796727'
      x-ratelimit-reset-requests:
      - 12ms
      x-ratelimit-reset-tokens:
      - 245ms
      x-request-id:
      - req_718ee466da35245b4badc6eded54a152
    status:
      code: 200
      message: OK
version: 1

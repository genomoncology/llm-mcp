interactions:
- request:
    body: '{"method":"initialize","params":{"protocolVersion":"2025-03-26","capabilities":{"sampling":{},"roots":{"listChanged":true}},"clientInfo":{"name":"mcp","version":"0.1.0"}},"jsonrpc":"2.0","id":0}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '194'
      content-type:
      - application/json
      host:
      - gitmcp.io
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":0,"result":{"protocolVersion":"2025-03-26","capabilities":{"tools":{"listChanged":true}},"serverInfo":{"name":"GitMCP","version":"1.1.0"}}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9447a78b0a198f44-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 21:30:27 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - ff525df2a65b4ec99836e1d96cc20a55cab928453633f195168add09a267795e
    status:
      code: 200
      message: OK
- request:
    body: '{"method":"notifications/initialized","jsonrpc":"2.0"}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '54'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - ff525df2a65b4ec99836e1d96cc20a55cab928453633f195168add09a267795e
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: ''
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9447a7cf6bf78f44-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 23 May 2025 21:30:37 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 202
      message: Accepted
- request:
    body: '{"method":"tools/list","jsonrpc":"2.0","id":1}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '46'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - ff525df2a65b4ec99836e1d96cc20a55cab928453633f195168add09a267795e
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":1,"result":{"tools":[{"name":"fetch_llm_documentation","description":"Fetch
        entire documentation file from GitHub repository: simonw/llm. Useful for general
        questions. Always call this tool first if asked about simonw/llm.","inputSchema":{"type":"object"},"annotations":{"_def":{"options":[{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"typeName":"ZodNull"},"~standard":{"version":1,"vendor":"zod"}}],"typeName":"ZodUnion"},"~standard":{"version":1,"vendor":"zod"}}},{"name":"search_llm_documentation","description":"Semantically
        search within the fetched documentation from GitHub repository: simonw/llm.
        Useful for specific queries.","inputSchema":{"type":"object","properties":{"query":{"type":"string","description":"The
        search query to find relevant documentation"}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},{"name":"search_llm_code","description":"Search
        for code within the GitHub repository: \"simonw/llm\" using the GitHub Search
        API (exact match). Returns matching files for you to query further if relevant.","inputSchema":{"type":"object","properties":{"query":{"type":"string","description":"The
        search query to find relevant code files"},"page":{"type":"number","description":"Page
        number to retrieve (starting from 1). Each page contains 30 results."}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},{"name":"fetch_generic_url_content","description":"Generic
        tool to fetch content from any absolute URL, respecting robots.txt rules.
        Use this to retrieve referenced urls (absolute urls) that were mentioned in
        previously fetched documentation.","inputSchema":{"type":"object","properties":{"url":{"type":"string","description":"The
        URL of the document or page to fetch"}},"required":["url"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}]}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9447a7cfbc648f44-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 21:30:37 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - ff525df2a65b4ec99836e1d96cc20a55cab928453633f195168add09a267795e
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - ff525df2a65b4ec99836e1d96cc20a55cab928453633f195168add09a267795e
      user-agent:
      - python-httpx/0.28.1
    method: DELETE
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: '{"jsonrpc":"2.0","error":{"code":-32000,"message":"Method not allowed"},"id":null}'
    headers:
      CF-RAY:
      - 9447a80e99d88f44-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '82'
      Content-Type:
      - text/plain;charset=UTF-8
      Date:
      - Fri, 23 May 2025 21:30:47 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 405
      message: Method Not Allowed
- request:
    body: '{"messages":[{"role":"user","content":"According to the `llm`, what are
      the aliases for the model named ''gpt-3.5-turbo-16k''? Respond **only** with
      JSON formatted as {\"aliases\": [\"alias1\", \"alias2\", ...]}."}],"model":"gpt-4.1","stream":false,"tools":[{"type":"function","function":{"name":"fetch_llm_documentation","description":"Fetch
      entire documentation file from GitHub repository: simonw/llm. Useful for general
      questions. Always call this tool first if asked about simonw/llm.","parameters":{}}},{"type":"function","function":{"name":"search_llm_documentation","description":"Semantically
      search within the fetched documentation from GitHub repository: simonw/llm.
      Useful for specific queries.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant documentation"}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"search_llm_code","description":"Search
      for code within the GitHub repository: \"simonw/llm\" using the GitHub Search
      API (exact match). Returns matching files for you to query further if relevant.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant code files"},"page":{"type":"number","description":"Page
      number to retrieve (starting from 1). Each page contains 30 results."}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"fetch_generic_url_content","description":"Generic
      tool to fetch content from any absolute URL, respecting robots.txt rules. Use
      this to retrieve referenced urls (absolute urls) that were mentioned in previously
      fetched documentation.","parameters":{"type":"object","properties":{"url":{"type":"string","description":"The
      URL of the document or page to fetch"}},"required":["url"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}]}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '2039'
      content-type:
      - application/json
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.79.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.79.0
      x-stainless-read-timeout:
      - '600'
      x-stainless-retry-count:
      - '0'
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.12.7
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAAwAAAP//jFPRbpswFH3nK6z7HKKQQNPwtqpbtT1Na7tsWirkmgvxYmzPNuuSKP8+
        YRIgbSaNB2T5+Jx77rn2PiAEeA4pAbamjlVahDf08e7r/IEub++WNyrKP34qb9893i90svuwgFHD
        UM8/kbkTa8xUpQU6rmQLM4PUYaMazePrySy5juceqFSOoqGV2oXxOAqnk2kSTuIwio/MteIMLaTk
        R0AIIXv/bzzKHP9ASiaj006F1tISIe0OEQJGiWYHqLXcOiodjHqQKelQNrZlLcQAcEqJjFEh+sLt
        tx+s+6CoENnGLnel2F59+/KwWe6+vyw+36v3LwUf1Gult9obKmrJuoAGeLefvipGCEhaea5Fatg6
        E6LKcsXqCqWjF7QIAWpKDzd9wH4Fv2o02xWkK6CCU4uWFMqQJvzZOAldbZ5VGF1tVnCAM6lDcGn9
        NIjMYFFbKt5mSaVUrT0f5tMROXRzE6rURj3bV1QouOR2nRmk1scxnEpwMuItQH02eNBGVdplTm3Q
        F51FSasK/cXs0eniCDrlqBiw4nh0QS/L0VHub0Z3GRlla8x7an8paZ1zNQCCQe9v3VzSbvvnsvwf
        +R5gDLXDPNMGc87OO+6PGWze7b+OdSl7w2DR/OYMM8fRNPPIsaC1aF8U2K11WGUFlyUabbh/VlDo
        LIkwmswnrJhCcAj+AgAA//8DAKro+1lgBAAA
    headers:
      CF-RAY:
      - 9447a80ffc72a3e2-ORD
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Fri, 23 May 2025 21:30:48 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=KVYRKP7hO6Jk1htCQeixTbM7NMsQ6hHVK9fzvoG1_MM-1748035848-1.0.1.1-3TZJRzR4Oba.C4MdtYhrcIimcesNpgexWDtz_BxsxfVen_L0jDkh9ylK69CbuafateGJu2ZitGHIG3hLnIiSg0rSUbmAxuYb_.LTaiGHqKw;
        path=/; expires=Fri, 23-May-25 22:00:48 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=TDD6v3G4QqTJSrlDh2d9APybvVmkEU65Vy1YVN.uRmA-1748035848371-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      access-control-expose-headers:
      - X-Request-ID
      alt-svc:
      - h3=":443"; ma=86400
      cf-cache-status:
      - DYNAMIC
      openai-organization:
      - genomoncology-2v7ur8
      openai-processing-ms:
      - '574'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=31536000; includeSubDomains; preload
      x-envoy-upstream-service-time:
      - '578'
      x-ratelimit-limit-requests:
      - '5000'
      x-ratelimit-limit-tokens:
      - '800000'
      x-ratelimit-remaining-requests:
      - '4999'
      x-ratelimit-remaining-tokens:
      - '799955'
      x-ratelimit-reset-requests:
      - 12ms
      x-ratelimit-reset-tokens:
      - 3ms
      x-request-id:
      - req_5e0219e9675b8ba3e5fef1fb2c518131
    status:
      code: 200
      message: OK
- request:
    body: '{"method":"initialize","params":{"protocolVersion":"2025-03-26","capabilities":{"sampling":{},"roots":{"listChanged":true}},"clientInfo":{"name":"mcp","version":"0.1.0"}},"jsonrpc":"2.0","id":0}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '194'
      content-type:
      - application/json
      host:
      - gitmcp.io
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":0,"result":{"protocolVersion":"2025-03-26","capabilities":{"tools":{"listChanged":true}},"serverInfo":{"name":"GitMCP","version":"1.1.0"}}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9447a81538fdeaf4-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 21:30:49 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - 23a0e9bc7513973826cf2c92330dde83304519cc0478ba29f41e578939cc0736
    status:
      code: 200
      message: OK
- request:
    body: '{"method":"notifications/initialized","jsonrpc":"2.0"}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '54'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - 23a0e9bc7513973826cf2c92330dde83304519cc0478ba29f41e578939cc0736
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: ''
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9447a8593e62eaf4-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 23 May 2025 21:30:59 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 202
      message: Accepted
- request:
    body: '{"method":"tools/call","params":{"name":"search_llm_documentation","arguments":{"query":"aliases
      for gpt-3.5-turbo-16k"}},"jsonrpc":"2.0","id":1}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '145'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - 23a0e9bc7513973826cf2c92330dde83304519cc0478ba29f41e578939cc0736
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":1,"result":{"searchQuery":"aliases for gpt-3.5-turbo-16k","content":[{"type":"text","text":"##
        Query\n\naliases for gpt-3.5-turbo-16k.\n\n## Response\n\n### Sources:\nImportant:
        you can fetch the full content of any source using the fetch_url_content tool\n\n####
        (simonw/llm/docs/openai-models.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/openai-models.md]
        (Score: 0.56)\n- \n## OpenAI completion models\n\nThe `gpt-3.5-turbo-instruct`
        model is a little different - it is a completion model rather than a chat
        model, described in [the OpenAI completions documentation](https://platform.openai.com/docs/api-reference/completions/create).\n\nCompletion
        models can be called with the `-o logprobs 3` option (not supported by chat
        models) which will cause LLM to store 3 log probabilities for each returned
        token in the SQLite database. Consult [this issue](https://github.com/simonw/llm/issues/284#issuecomment-1724772704)
        for details on how to read these values.\n\n(openai-extra-models)=\n\n\n-
        \n## OpenAI language models\n\nRun `llm models` for a full list of available
        models. The OpenAI models supported by LLM are:\n\n<!-- [[[cog\nfrom click.testing
        import CliRunner\nfrom llm.cli import cli\nresult = CliRunner().invoke(cli,
        [\"models\", \"list\"])\nmodels = [line for line in result.output.split(\"\\n\")
        if line.startswith(\"OpenAI \")]\ncog.out(\"```\\n{}\\n```\".format(\"\\n\".join(models)))\n]]]
        -->\n```\nOpenAI Chat: gpt-4o (aliases: 4o)\nOpenAI Chat: chatgpt-4o-latest
        (aliases: chatgpt-4o)\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\nOpenAI
        Chat: gpt-4o-audio-preview\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\nOpenAI
        Chat: gpt-4o-audio-preview-2024-10-01\nOpenAI Chat: gpt-4o-mini-audio-preview\nOpenAI
        Chat: gpt-4o-mini-audio-preview-2024-12-17\nOpenAI Chat: gpt-4.1 (aliases:
        4.1)\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\nOpenAI Chat: gpt-4.1-nano
        (aliases: 4.1-nano)\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\nOpenAI
        Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\nOpenAI Chat: gpt-4
        (aliases: 4, gpt4)\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\nOpenAI Chat:
        gpt-4-1106-preview\nOpenAI Chat: gpt-4-0125-preview\nOpenAI Chat: gpt-4-turbo-2024-04-09\nOpenAI
        Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\nOpenAI Chat:
        gpt-4.5-preview-2025-02-27\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\nOpenAI
        Chat: o1\nOpenAI Chat: o1-2024-12-17\nOpenAI Chat: o1-preview\nOpenAI Chat:
        o1-mini\nOpenAI Chat: o3-mini\nOpenAI Chat: o3\nOpenAI Chat: o4-mini\nOpenAI
        Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n```\n<!--
        [[[end]]] -->\n\nSee [the OpenAI models documentation](https://platform.openai.com/docs/models)
        for details of each of these.\n\n`gpt-4o-mini` (aliased to `4o-mini`) is the
        least expensive model, and is the default for if you don''t specify a model
        at all. Consult [OpenAI''s model documentation](https://platform.openai.com/docs/models)
        for details of the other models.\n\n\n#### (simonw/llm/llms.txt)[https://raw.githubusercontent.com/simonw/llm/main/llms.txt]
        (Score: 0.56)\n- URLs as input to the model.\n- **System Prompts**: Set a
        system prompt to influence the model''s responses.\n\nKey technical terms
        and topics extracted from the document:\n- LLM command line tool\n- Large
        language models (LLMs)\n- Model options and configuration\n- Attachments and
        multi-modal input\n- System prompts and conversation management\n- Default
        options for models\n- Prompt templates\n- Tool execution and external functions\n-
        Logging and conversation history\n\n\n\n#\n- t.upper()\n\nmodel = llm.get_model(\"gpt-4.1-mini\")\nresponse
        = model.prompt(\"Convert panda to upper\", tools=[upper])\ntool_calls = response.tool_calls()\n```\n\nKey
        technical terms and topics include:\n\n- **LLM**: Large Language Model\n-
        **Model**: A specific LLM instance, e.g., GPT-4o-mini\n- **Prompt**: The input
        text to the model\n- **Response**: The output from the model\n- **Tools**:
        Functions that can be executed by the model\n- **Attachments**: Files or URLs
        that can be passed to the model\n- **Schemas**: JSON schema definitions for
        the response\n- **Conversations**: Ongoing discussions with the model\n\nThe
        API supports various features, including:\n\n* **Lazy Loading**: Responses
        are not evaluated until the `text()` method is called\n* **Async Models**:
        Support for asynchronous models using `llm.get_async_model()`\n* **Streaming
        Responses**: Support for streaming responses as they are generated\n* **Token
        Usage**: Methods for tracking token usage, e.g., `response.usage()` \n\nThe
        document also lists several functions for managing models, aliases, and defaults:\n\n*
        `llm.set_alias(alias, model_id)`: Define a new alias\n* `llm.remove_alias(alias)`:
        Remove an alias\n* `llm.set_default_model(model_id)`: Set the default model\n*
        `llm.get_default_model()`: Get the currently configured default model\n\n\n-
        [related-tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/related-tools.md):
        Related Tools for Working with LLMs\nThis document describes tools designed
        to be used with Large Language Models (LLMs). It covers three main tools:
        `strip-tags`, `ttok`, and `symbex`. `strip-tags` is a command for stripping
        HTML tags, useful for working within token budget limits. `ttok` counts OpenAI
        tokens and can truncate input to a desired number of tokens. `symbex` searches
        for symbols in Python codebases, allowing for code extraction and piping into
        LLMs for various tasks. These tools enhance the functionality and usability
        of LLMs in different applications.\n\nExample usage of `strip-tags` and `ttok`:\n```bash\ncurl
        -s https://www.nytimes.com/ \\\n\n- becomes the description that is passed
        to the model.\n\nThe document also mentions that extra examples of using tools
        can be found for the [command-line tool](usage-tools) and [Python API](python-api-tools).\n\nKEY
        TERMS:\n- Tools\n- Large Language Models\n- Functions\n- Python API\n- Command-line
        interface\n- Plugin hooks\n- Tool execution\n- Model prompts\n- Function definitions\n-
        Docstring \n\nLet me know if I can help with anything else.\n\n\n- [usage.md](https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md):
        LLM Command Line Usage and Options\nThe document provides a comprehensive
        guide to using the LLM command line tool, which allows users to interact with
        large language models (LLMs) from the command line. The tool supports various
        models, including OpenAI''s GPT-4o and GPT-4o-mini, and enables features like
        text generation, conversation continuation, and tool execution. Users can
        run prompts, list available models, and configure default options for specific
        models. The document covers usage examples, model options, attachments, system
        prompts, and conversation management.\n\nTo run a prompt, users can simply
        type `llm ''their prompt''`. The tool also supports executing prompts with
        specific models, such as `llm -m gpt-4o ''Ten names for cheesecakes''`. \n\nExample
        usage of listing models:\n```bash\nllm models\n```\n\nExample usage of setting
        default options for a model:\n```bash\nllm models options set gpt-4o temperature
        0.5\n```\n\nThe document also explains how to use system prompts, which allow
        users to set a specific tone or style for the model''s responses. \n\nSome
        key features of the tool include:\n- **Prompt Execution**: Run prompts with
        specific models, e.g., `llm -m gpt-4o ''Ten names for cheesecakes''`.\n- **Model
        Options**: Configure model-specific options, such as temperature and max tokens.\n-
        **Attachments**: Pass files or URLs as input to the model.\n- **System Prompts**:
        Set a system prompt to influence the model''s responses.\n\nKey technical
        terms and topics extracted from the document:\n- LLM command line tool\n-
        Large languag\n- \nllm --system ''Summarize this'' --model gpt-4o --save summarize\n```\n\n**KEY
        TECHNICAL TERMS:**\n\n1. **Templates**: Reusable units that combine prompts,
        system prompts, models, and schema.\n2. **--save**: Option to create a template.\n3.
        **-t/--template**: Option to execute a named template.\n4. **YAML files**:
        Templates are stored as YAML files on disk.\n5. **System prompts**: Can be
        set using the `system:` key in a template.\n6. **Fragments**: Can be referenced
        in templates using the `fragments:` and `system_fragments:` keys.\n7. **Options**:
        Default options can be set using the `options:` key in a template.\n8. **Schemas**:
        Can be embedded in templates using the `schema_object:` key.\n9. **Template
        loaders**: Plugins can register prefixes to load templates from external sources.\n\n\n-
        [tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md):
        Tools in LLM for Executing Functions\nThe document explains how Large Language
        Models (LLMs) can execute tools as part of their responses. A tool is a function
        that the model can request to be executed, with a list of available tools
        provided in the initial prompt. The model can then choose to call one or more
        tools, and the code executes the specified tool with the provided arguments.
        The output of the tool execution is then included in the model''s next prompt.
        LLM implements tools as Python functions, which can take any number of arguments
        and return a string or an object that can be converted to a string. The functions
        can be defined directly in the Python API or via plugins/command-line arguments.
        Tools can be used with the LLM command-line tool or the Python API.\n\nFor
        example, a tool function in Python can be defined as:\n```python\ndef greet(name:
        str) -> str:\n    \"\"\"Return a personalized greeting.\"\"\"\n    return
        f\"Hello, {name}!\"\n```\nThe docstring of the function becomes the description
        that is passed to the model.\n\nThe document also mentions that extra examples
        of using tools can be found for the [command-line tool](usage-tools) and [Python
        API](python-api-tools).\n\n\n#### (simonw/llm/docs/tools.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md]
        (Score: 0.56)\n- \n\nMany Large Language Models have been trained to execute
        tools as part of responding to a prompt. LLM supports tool usage with both
        the command-line interface and the Python API.\n\n#\n\n#### (simonw/llm/docs/schemas.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/schemas.md]
        (Score: 0.54)\n- \n\nLarge Language Models are very good at producing structured
        output as JSON or other formats. LLM''s **schemas** feature allows you to
        define the exact structure of JSON data you want to receive from a model.\n\nThis
        feature is supported by models from OpenAI, Anthropic, Google Gemini and can
        be implemented for others {ref}`via plugins <advanced-model-plugins-schemas>`.\n\nThis
        page describes schemas used via the `llm` command-line tool. Schemas can also
        be used from the {ref}`Python API <python-api-schemas>`.\n\n(schemas-tutorial)=\n\n#\n\n####
        (simonw/llm/docs/usage.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md]
        (Score: 0.53)\n- \n\nSome models are completion models - rather than being
        tuned to respond to chat style prompts, they are designed to complete a sentence
        or paragraph.\n\nAn example of this is the `gpt-3.5-turbo-instruct` OpenAI
        model.\n\nYou can prompt that model the same way as the chat models, but be
        aware that the prompt format that works best is likely to differ.\n\n```bash\nllm
        -m gpt-3.5-turbo-instruct ''Reasons to tame a wild beaver:''\n```\n\n(usage-chat)=\n\n\n\n####
        (simonw/llm/docs/aliases.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/aliases.md]
        (Score: 0.52)\n- \n# Model aliases\n\nLLM supports model aliases, which allow
        you to refer to a model by a short name instead of its full ID.\n\n## Listing
        aliases\n\nTo list current aliases, run this:\n\n```bash\nllm aliases\n```\nExample
        output:\n\n<!-- [[[cog\nfrom click.testing import CliRunner\nfrom llm.cli
        import cli\nresult = CliRunner().invoke(cli, [\"aliases\", \"list\"])\ncog.out(\"```\\n{}```\".format(result.output))\n]]]
        -->\n```\n4o                  : gpt-4o\nchatgpt-4o          : chatgpt-4o-latest\n4o-mini             :
        gpt-4o-mini\n4.1                 : gpt-4.1\n4.1-mini            : gpt-4.1-mini\n4.1-nano            :
        gpt-4.1-nano\n3.5                 : gpt-3.5-turbo\nchatgpt             : gpt-3.5-turbo\nchatgpt-16k         :
        gpt-3.5-turbo-16k\n3.5-16k             : gpt-3.5-turbo-16k\n4                   :
        gpt-4\ngpt4                : gpt-4\n4-32k               : gpt-4-32k\ngpt-4-turbo-preview
        : gpt-4-turbo\n4-turbo             : gpt-4-turbo\n4t                  : gpt-4-turbo\ngpt-4.5             :
        gpt-4.5-preview\n3.5-instruct        : gpt-3.5-turbo-instruct\nchatgpt-instruct    :
        gpt-3.5-turbo-instruct\nada                 : text-embedding-ada-002 (embedding)\nada-002             :
        text-embedding-ada-002 (embedding)\n3-small             : text-embedding-3-small
        (embedding)\n3-large             : text-embedding-3-large (embedding)\n3-small-512         :
        text-embedding-3-small-512 (embedding)\n3-large-256         : text-embedding-3-large-256
        (embedding)\n3-large-1024        : text-embedding-3-large-1024 (embedding)\n```\n<!--
        [[[end]]] -->\n\nAdd `--json` to get that list back as JSON:\n\n```bash\nllm
        aliases list --json\n```\nExample output:\n```json\n{\n    \"3.5\": \"gpt-3.5-turbo\",\n    \"chatgpt\":
        \"gpt-3.5-turbo\",\n    \"4\": \"gpt-4\",\n    \"gpt4\": \"gpt-4\",\n    \"ada\":
        \"ada-002\"\n}\n```\n\n\n"}]}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9447a8598f70eaf4-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 21:30:59 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - 23a0e9bc7513973826cf2c92330dde83304519cc0478ba29f41e578939cc0736
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - 23a0e9bc7513973826cf2c92330dde83304519cc0478ba29f41e578939cc0736
      user-agent:
      - python-httpx/0.28.1
    method: DELETE
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: '{"jsonrpc":"2.0","error":{"code":-32000,"message":"Method not allowed"},"id":null}'
    headers:
      CF-RAY:
      - 9447a8b56b47eaf4-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '82'
      Content-Type:
      - text/plain;charset=UTF-8
      Date:
      - Fri, 23 May 2025 21:31:14 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 405
      message: Method Not Allowed
- request:
    body: '{"messages":[{"role":"user","content":"According to the `llm`, what are
      the aliases for the model named ''gpt-3.5-turbo-16k''? Respond **only** with
      JSON formatted as {\"aliases\": [\"alias1\", \"alias2\", ...]}."},{"role":"assistant","tool_calls":[{"type":"function","id":"call_ksWzgly6XRTkWzYw9PSoEwfi","function":{"name":"search_llm_documentation","arguments":"{\"query\":
      \"aliases for gpt-3.5-turbo-16k\"}"}}]},{"role":"tool","tool_call_id":"call_ksWzgly6XRTkWzYw9PSoEwfi","content":"##
      Query\n\naliases for gpt-3.5-turbo-16k.\n\n## Response\n\n### Sources:\nImportant:
      you can fetch the full content of any source using the fetch_url_content tool\n\n####
      (simonw/llm/docs/openai-models.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/openai-models.md]
      (Score: 0.56)\n- \n## OpenAI completion models\n\nThe `gpt-3.5-turbo-instruct`
      model is a little different - it is a completion model rather than a chat model,
      described in [the OpenAI completions documentation](https://platform.openai.com/docs/api-reference/completions/create).\n\nCompletion
      models can be called with the `-o logprobs 3` option (not supported by chat
      models) which will cause LLM to store 3 log probabilities for each returned
      token in the SQLite database. Consult [this issue](https://github.com/simonw/llm/issues/284#issuecomment-1724772704)
      for details on how to read these values.\n\n(openai-extra-models)=\n\n\n- \n##
      OpenAI language models\n\nRun `llm models` for a full list of available models.
      The OpenAI models supported by LLM are:\n\n<!-- [[[cog\nfrom click.testing import
      CliRunner\nfrom llm.cli import cli\nresult = CliRunner().invoke(cli, [\"models\",
      \"list\"])\nmodels = [line for line in result.output.split(\"\\n\") if line.startswith(\"OpenAI
      \")]\ncog.out(\"```\\n{}\\n```\".format(\"\\n\".join(models)))\n]]] -->\n```\nOpenAI
      Chat: gpt-4o (aliases: 4o)\nOpenAI Chat: chatgpt-4o-latest (aliases: chatgpt-4o)\nOpenAI
      Chat: gpt-4o-mini (aliases: 4o-mini)\nOpenAI Chat: gpt-4o-audio-preview\nOpenAI
      Chat: gpt-4o-audio-preview-2024-12-17\nOpenAI Chat: gpt-4o-audio-preview-2024-10-01\nOpenAI
      Chat: gpt-4o-mini-audio-preview\nOpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\nOpenAI
      Chat: gpt-4.1 (aliases: 4.1)\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\nOpenAI
      Chat: gpt-4.1-nano (aliases: 4.1-nano)\nOpenAI Chat: gpt-3.5-turbo (aliases:
      3.5, chatgpt)\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\nOpenAI
      Chat: gpt-4 (aliases: 4, gpt4)\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\nOpenAI
      Chat: gpt-4-1106-preview\nOpenAI Chat: gpt-4-0125-preview\nOpenAI Chat: gpt-4-turbo-2024-04-09\nOpenAI
      Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\nOpenAI Chat:
      gpt-4.5-preview-2025-02-27\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\nOpenAI
      Chat: o1\nOpenAI Chat: o1-2024-12-17\nOpenAI Chat: o1-preview\nOpenAI Chat:
      o1-mini\nOpenAI Chat: o3-mini\nOpenAI Chat: o3\nOpenAI Chat: o4-mini\nOpenAI
      Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n```\n<!--
      [[[end]]] -->\n\nSee [the OpenAI models documentation](https://platform.openai.com/docs/models)
      for details of each of these.\n\n`gpt-4o-mini` (aliased to `4o-mini`) is the
      least expensive model, and is the default for if you don''t specify a model
      at all. Consult [OpenAI''s model documentation](https://platform.openai.com/docs/models)
      for details of the other models.\n\n\n#### (simonw/llm/llms.txt)[https://raw.githubusercontent.com/simonw/llm/main/llms.txt]
      (Score: 0.56)\n- URLs as input to the model.\n- **System Prompts**: Set a system
      prompt to influence the model''s responses.\n\nKey technical terms and topics
      extracted from the document:\n- LLM command line tool\n- Large language models
      (LLMs)\n- Model options and configuration\n- Attachments and multi-modal input\n-
      System prompts and conversation management\n- Default options for models\n-
      Prompt templates\n- Tool execution and external functions\n- Logging and conversation
      history\n\n\n\n#\n- t.upper()\n\nmodel = llm.get_model(\"gpt-4.1-mini\")\nresponse
      = model.prompt(\"Convert panda to upper\", tools=[upper])\ntool_calls = response.tool_calls()\n```\n\nKey
      technical terms and topics include:\n\n- **LLM**: Large Language Model\n- **Model**:
      A specific LLM instance, e.g., GPT-4o-mini\n- **Prompt**: The input text to
      the model\n- **Response**: The output from the model\n- **Tools**: Functions
      that can be executed by the model\n- **Attachments**: Files or URLs that can
      be passed to the model\n- **Schemas**: JSON schema definitions for the response\n-
      **Conversations**: Ongoing discussions with the model\n\nThe API supports various
      features, including:\n\n* **Lazy Loading**: Responses are not evaluated until
      the `text()` method is called\n* **Async Models**: Support for asynchronous
      models using `llm.get_async_model()`\n* **Streaming Responses**: Support for
      streaming responses as they are generated\n* **Token Usage**: Methods for tracking
      token usage, e.g., `response.usage()` \n\nThe document also lists several functions
      for managing models, aliases, and defaults:\n\n* `llm.set_alias(alias, model_id)`:
      Define a new alias\n* `llm.remove_alias(alias)`: Remove an alias\n* `llm.set_default_model(model_id)`:
      Set the default model\n* `llm.get_default_model()`: Get the currently configured
      default model\n\n\n- [related-tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/related-tools.md):
      Related Tools for Working with LLMs\nThis document describes tools designed
      to be used with Large Language Models (LLMs). It covers three main tools: `strip-tags`,
      `ttok`, and `symbex`. `strip-tags` is a command for stripping HTML tags, useful
      for working within token budget limits. `ttok` counts OpenAI tokens and can
      truncate input to a desired number of tokens. `symbex` searches for symbols
      in Python codebases, allowing for code extraction and piping into LLMs for various
      tasks. These tools enhance the functionality and usability of LLMs in different
      applications.\n\nExample usage of `strip-tags` and `ttok`:\n```bash\ncurl -s
      https://www.nytimes.com/ \\\n\n- becomes the description that is passed to the
      model.\n\nThe document also mentions that extra examples of using tools can
      be found for the [command-line tool](usage-tools) and [Python API](python-api-tools).\n\nKEY
      TERMS:\n- Tools\n- Large Language Models\n- Functions\n- Python API\n- Command-line
      interface\n- Plugin hooks\n- Tool execution\n- Model prompts\n- Function definitions\n-
      Docstring \n\nLet me know if I can help with anything else.\n\n\n- [usage.md](https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md):
      LLM Command Line Usage and Options\nThe document provides a comprehensive guide
      to using the LLM command line tool, which allows users to interact with large
      language models (LLMs) from the command line. The tool supports various models,
      including OpenAI''s GPT-4o and GPT-4o-mini, and enables features like text generation,
      conversation continuation, and tool execution. Users can run prompts, list available
      models, and configure default options for specific models. The document covers
      usage examples, model options, attachments, system prompts, and conversation
      management.\n\nTo run a prompt, users can simply type `llm ''their prompt''`.
      The tool also supports executing prompts with specific models, such as `llm
      -m gpt-4o ''Ten names for cheesecakes''`. \n\nExample usage of listing models:\n```bash\nllm
      models\n```\n\nExample usage of setting default options for a model:\n```bash\nllm
      models options set gpt-4o temperature 0.5\n```\n\nThe document also explains
      how to use system prompts, which allow users to set a specific tone or style
      for the model''s responses. \n\nSome key features of the tool include:\n- **Prompt
      Execution**: Run prompts with specific models, e.g., `llm -m gpt-4o ''Ten names
      for cheesecakes''`.\n- **Model Options**: Configure model-specific options,
      such as temperature and max tokens.\n- **Attachments**: Pass files or URLs as
      input to the model.\n- **System Prompts**: Set a system prompt to influence
      the model''s responses.\n\nKey technical terms and topics extracted from the
      document:\n- LLM command line tool\n- Large languag\n- \nllm --system ''Summarize
      this'' --model gpt-4o --save summarize\n```\n\n**KEY TECHNICAL TERMS:**\n\n1.
      **Templates**: Reusable units that combine prompts, system prompts, models,
      and schema.\n2. **--save**: Option to create a template.\n3. **-t/--template**:
      Option to execute a named template.\n4. **YAML files**: Templates are stored
      as YAML files on disk.\n5. **System prompts**: Can be set using the `system:`
      key in a template.\n6. **Fragments**: Can be referenced in templates using the
      `fragments:` and `system_fragments:` keys.\n7. **Options**: Default options
      can be set using the `options:` key in a template.\n8. **Schemas**: Can be embedded
      in templates using the `schema_object:` key.\n9. **Template loaders**: Plugins
      can register prefixes to load templates from external sources.\n\n\n- [tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md):
      Tools in LLM for Executing Functions\nThe document explains how Large Language
      Models (LLMs) can execute tools as part of their responses. A tool is a function
      that the model can request to be executed, with a list of available tools provided
      in the initial prompt. The model can then choose to call one or more tools,
      and the code executes the specified tool with the provided arguments. The output
      of the tool execution is then included in the model''s next prompt. LLM implements
      tools as Python functions, which can take any number of arguments and return
      a string or an object that can be converted to a string. The functions can be
      defined directly in the Python API or via plugins/command-line arguments. Tools
      can be used with the LLM command-line tool or the Python API.\n\nFor example,
      a tool function in Python can be defined as:\n```python\ndef greet(name: str)
      -> str:\n    \"\"\"Return a personalized greeting.\"\"\"\n    return f\"Hello,
      {name}!\"\n```\nThe docstring of the function becomes the description that is
      passed to the model.\n\nThe document also mentions that extra examples of using
      tools can be found for the [command-line tool](usage-tools) and [Python API](python-api-tools).\n\n\n####
      (simonw/llm/docs/tools.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md]
      (Score: 0.56)\n- \n\nMany Large Language Models have been trained to execute
      tools as part of responding to a prompt. LLM supports tool usage with both the
      command-line interface and the Python API.\n\n#\n\n#### (simonw/llm/docs/schemas.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/schemas.md]
      (Score: 0.54)\n- \n\nLarge Language Models are very good at producing structured
      output as JSON or other formats. LLM''s **schemas** feature allows you to define
      the exact structure of JSON data you want to receive from a model.\n\nThis feature
      is supported by models from OpenAI, Anthropic, Google Gemini and can be implemented
      for others {ref}`via plugins <advanced-model-plugins-schemas>`.\n\nThis page
      describes schemas used via the `llm` command-line tool. Schemas can also be
      used from the {ref}`Python API <python-api-schemas>`.\n\n(schemas-tutorial)=\n\n#\n\n####
      (simonw/llm/docs/usage.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md]
      (Score: 0.53)\n- \n\nSome models are completion models - rather than being tuned
      to respond to chat style prompts, they are designed to complete a sentence or
      paragraph.\n\nAn example of this is the `gpt-3.5-turbo-instruct` OpenAI model.\n\nYou
      can prompt that model the same way as the chat models, but be aware that the
      prompt format that works best is likely to differ.\n\n```bash\nllm -m gpt-3.5-turbo-instruct
      ''Reasons to tame a wild beaver:''\n```\n\n(usage-chat)=\n\n\n\n#### (simonw/llm/docs/aliases.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/aliases.md]
      (Score: 0.52)\n- \n# Model aliases\n\nLLM supports model aliases, which allow
      you to refer to a model by a short name instead of its full ID.\n\n## Listing
      aliases\n\nTo list current aliases, run this:\n\n```bash\nllm aliases\n```\nExample
      output:\n\n<!-- [[[cog\nfrom click.testing import CliRunner\nfrom llm.cli import
      cli\nresult = CliRunner().invoke(cli, [\"aliases\", \"list\"])\ncog.out(\"```\\n{}```\".format(result.output))\n]]]
      -->\n```\n4o                  : gpt-4o\nchatgpt-4o          : chatgpt-4o-latest\n4o-mini             :
      gpt-4o-mini\n4.1                 : gpt-4.1\n4.1-mini            : gpt-4.1-mini\n4.1-nano            :
      gpt-4.1-nano\n3.5                 : gpt-3.5-turbo\nchatgpt             : gpt-3.5-turbo\nchatgpt-16k         :
      gpt-3.5-turbo-16k\n3.5-16k             : gpt-3.5-turbo-16k\n4                   :
      gpt-4\ngpt4                : gpt-4\n4-32k               : gpt-4-32k\ngpt-4-turbo-preview
      : gpt-4-turbo\n4-turbo             : gpt-4-turbo\n4t                  : gpt-4-turbo\ngpt-4.5             :
      gpt-4.5-preview\n3.5-instruct        : gpt-3.5-turbo-instruct\nchatgpt-instruct    :
      gpt-3.5-turbo-instruct\nada                 : text-embedding-ada-002 (embedding)\nada-002             :
      text-embedding-ada-002 (embedding)\n3-small             : text-embedding-3-small
      (embedding)\n3-large             : text-embedding-3-large (embedding)\n3-small-512         :
      text-embedding-3-small-512 (embedding)\n3-large-256         : text-embedding-3-large-256
      (embedding)\n3-large-1024        : text-embedding-3-large-1024 (embedding)\n```\n<!--
      [[[end]]] -->\n\nAdd `--json` to get that list back as JSON:\n\n```bash\nllm
      aliases list --json\n```\nExample output:\n```json\n{\n    \"3.5\": \"gpt-3.5-turbo\",\n    \"chatgpt\":
      \"gpt-3.5-turbo\",\n    \"4\": \"gpt-4\",\n    \"gpt4\": \"gpt-4\",\n    \"ada\":
      \"ada-002\"\n}\n```\n\n\n"}],"model":"gpt-4.1","stream":false,"tools":[{"type":"function","function":{"name":"fetch_llm_documentation","description":"Fetch
      entire documentation file from GitHub repository: simonw/llm. Useful for general
      questions. Always call this tool first if asked about simonw/llm.","parameters":{}}},{"type":"function","function":{"name":"search_llm_documentation","description":"Semantically
      search within the fetched documentation from GitHub repository: simonw/llm.
      Useful for specific queries.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant documentation"}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"search_llm_code","description":"Search
      for code within the GitHub repository: \"simonw/llm\" using the GitHub Search
      API (exact match). Returns matching files for you to query further if relevant.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant code files"},"page":{"type":"number","description":"Page
      number to retrieve (starting from 1). Each page contains 30 results."}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"fetch_generic_url_content","description":"Generic
      tool to fetch content from any absolute URL, respecting robots.txt rules. Use
      this to retrieve referenced urls (absolute urls) that were mentioned in previously
      fetched documentation.","parameters":{"type":"object","properties":{"url":{"type":"string","description":"The
      URL of the document or page to fetch"}},"required":["url"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}]}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '15595'
      content-type:
      - application/json
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.79.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.79.0
      x-stainless-read-timeout:
      - '600'
      x-stainless-retry-count:
      - '0'
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.12.7
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAAwAAAP//jFJBbtswELzrFcSeJUOS5cjxrb20BdL2ZKBoZQgMuZLZUCRBUk0Lw38v
        SMWW0iZALgTI2RnO7O4pIQQEhx0BdqSeDUZm7+n+w+PHzu6/33y7+7znfHv3ztRfy8cvn0oNaWDo
        +5/I/IW1YnowEr3QaoKZReoxqBZ1tc3Xm21dRWDQHGWg9cZn1arIyrzcZHmVFdUT86gFQwc78iMh
        hJBTPINHxfE37EieXl4GdI72CLtrESFgtQwvQJ0TzlPlIZ1BppVHFW2fGqBSUIeuCX81MUcwVdw8
        NJCSBtarzXQ5nJcaFrvR0ZBBjVIuAKqU9jT0ILo/PCHnq1+pe2P1vfuHCp1Qwh1bi9RpFbw5rw1E
        9JwQcoh9GZ9FBWP1YHzr9QPG79bbupgEYZ7FDJcX0GtP5ZJ2W6YvKLYcPRXSLXoLjLIj8pk7D4KO
        XOgFkCxy/2/nJe0pu1D9W+RngDE0HnlrLHLBnkeeyyyGXX2t7NrnaBgc2l+CYesF2jALjh0d5bRF
        4P44j0PbCdWjNVZMq9SZdlNgkdc560pIzslfAAAA//8DAPibtUdUAwAA
    headers:
      CF-RAY:
      - 9447a8b5ec2fe82d-ORD
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Fri, 23 May 2025 21:31:14 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=1P0S7OZOSHWj6qOIDvpVEv0lNu9ZGFcdjw3xgHeaeFA-1748035874-1.0.1.1-rA21Xawo_vyvYQlArbalTpRsVo9_9hLfWqf2x8cmiHHHhY3rIGvv2Wh7Z3teZelbrBri.xgE7B_YfdnnFQ6GBJmWoBktBFIhk7IQLNR2FPk;
        path=/; expires=Fri, 23-May-25 22:01:14 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=mqZbLILI3mRKSD6trUdb9HPj8Bfve46OGrTUzSpJpLk-1748035874989-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      access-control-expose-headers:
      - X-Request-ID
      alt-svc:
      - h3=":443"; ma=86400
      cf-cache-status:
      - DYNAMIC
      openai-organization:
      - genomoncology-2v7ur8
      openai-processing-ms:
      - '656'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=31536000; includeSubDomains; preload
      x-envoy-upstream-service-time:
      - '659'
      x-ratelimit-limit-requests:
      - '5000'
      x-ratelimit-limit-tokens:
      - '800000'
      x-ratelimit-remaining-requests:
      - '4999'
      x-ratelimit-remaining-tokens:
      - '796725'
      x-ratelimit-reset-requests:
      - 12ms
      x-ratelimit-reset-tokens:
      - 245ms
      x-request-id:
      - req_22bcf174d7e2831243621a5423592d58
    status:
      code: 200
      message: OK
version: 1

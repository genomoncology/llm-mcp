interactions:
- request:
    body: '{"method":"initialize","params":{"protocolVersion":"2025-03-26","capabilities":{"sampling":{},"roots":{"listChanged":true}},"clientInfo":{"name":"mcp","version":"0.1.0"}},"jsonrpc":"2.0","id":0}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '194'
      content-type:
      - application/json
      host:
      - gitmcp.io
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":0,"result":{"protocolVersion":"2025-03-26","capabilities":{"tools":{"listChanged":true}},"serverInfo":{"name":"GitMCP","version":"1.1.0"}}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9445bfc01dc27e6b-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 15:57:27 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - c2135902c10791fa1a453d19919654cb415b1297d04a7ac616d5e484f2c2f8d4
    status:
      code: 200
      message: OK
- request:
    body: '{"method":"notifications/initialized","jsonrpc":"2.0"}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '54'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - c2135902c10791fa1a453d19919654cb415b1297d04a7ac616d5e484f2c2f8d4
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: ''
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9445c0036f767e6b-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 23 May 2025 15:57:37 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 202
      message: Accepted
- request:
    body: '{"method":"tools/list","jsonrpc":"2.0","id":1}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '46'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - c2135902c10791fa1a453d19919654cb415b1297d04a7ac616d5e484f2c2f8d4
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":1,"result":{"tools":[{"name":"fetch_llm_documentation","description":"Fetch
        entire documentation file from GitHub repository: simonw/llm. Useful for general
        questions. Always call this tool first if asked about simonw/llm.","inputSchema":{"type":"object"},"annotations":{"_def":{"options":[{"_def":{"unknownKeys":"strip","catchall":{"_def":{"typeName":"ZodNever"},"~standard":{"version":1,"vendor":"zod"}},"typeName":"ZodObject"},"~standard":{"version":1,"vendor":"zod"},"_cached":null},{"_def":{"typeName":"ZodNull"},"~standard":{"version":1,"vendor":"zod"}}],"typeName":"ZodUnion"},"~standard":{"version":1,"vendor":"zod"}}},{"name":"search_llm_documentation","description":"Semantically
        search within the fetched documentation from GitHub repository: simonw/llm.
        Useful for specific queries.","inputSchema":{"type":"object","properties":{"query":{"type":"string","description":"The
        search query to find relevant documentation"}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},{"name":"search_llm_code","description":"Search
        for code within the GitHub repository: \"simonw/llm\" using the GitHub Search
        API (exact match). Returns matching files for you to query further if relevant.","inputSchema":{"type":"object","properties":{"query":{"type":"string","description":"The
        search query to find relevant code files"},"page":{"type":"number","description":"Page
        number to retrieve (starting from 1). Each page contains 30 results."}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}},{"name":"fetch_generic_url_content","description":"Generic
        tool to fetch content from any absolute URL, respecting robots.txt rules.
        Use this to retrieve referenced urls (absolute urls) that were mentioned in
        previously fetched documentation.","inputSchema":{"type":"object","properties":{"url":{"type":"string","description":"The
        URL of the document or page to fetch"}},"required":["url"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}]}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9445c003a8377e6b-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 15:57:37 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - c2135902c10791fa1a453d19919654cb415b1297d04a7ac616d5e484f2c2f8d4
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - c2135902c10791fa1a453d19919654cb415b1297d04a7ac616d5e484f2c2f8d4
      user-agent:
      - python-httpx/0.28.1
    method: DELETE
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: '{"jsonrpc":"2.0","error":{"code":-32000,"message":"Method not allowed"},"id":null}'
    headers:
      CF-RAY:
      - 9445c0429b047e6b-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '82'
      Content-Type:
      - text/plain;charset=UTF-8
      Date:
      - Fri, 23 May 2025 15:57:47 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 405
      message: Method Not Allowed
- request:
    body: '{"messages":[{"role":"user","content":"According to the `llm`, what are
      the aliases for the model named ''gpt-3.5-turbo-16k''? Respond **only** with
      JSON formatted as {\"aliases\": [\"alias1\", \"alias2\", ...]}."}],"model":"gpt-4.1","stream":false,"tools":[{"type":"function","function":{"name":"fetch_llm_documentation","description":"Fetch
      entire documentation file from GitHub repository: simonw/llm. Useful for general
      questions. Always call this tool first if asked about simonw/llm.","parameters":{}}},{"type":"function","function":{"name":"search_llm_documentation","description":"Semantically
      search within the fetched documentation from GitHub repository: simonw/llm.
      Useful for specific queries.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant documentation"}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"search_llm_code","description":"Search
      for code within the GitHub repository: \"simonw/llm\" using the GitHub Search
      API (exact match). Returns matching files for you to query further if relevant.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"The
      search query to find relevant code files"},"page":{"type":"number","description":"Page
      number to retrieve (starting from 1). Each page contains 30 results."}},"required":["query"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}},{"type":"function","function":{"name":"fetch_generic_url_content","description":"Generic
      tool to fetch content from any absolute URL, respecting robots.txt rules. Use
      this to retrieve referenced urls (absolute urls) that were mentioned in previously
      fetched documentation.","parameters":{"type":"object","properties":{"url":{"type":"string","description":"The
      URL of the document or page to fetch"}},"required":["url"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}}}]}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '2039'
      content-type:
      - application/json
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.79.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.79.0
      x-stainless-read-timeout:
      - '600'
      x-stainless-retry-count:
      - '0'
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.12.7
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAAwAAAP//jFPBjtMwEL3nK6w5N1XTTUu3N0BaQIsEYtkDoqtoOpkkbh072A6wW/Xf
        V0naJt0tEjlElp/fmzdv7F0gBMgUlgKoQE9lpcJ3+DX+QIu7z/P7m+Ipp83bDc3d5n7tv20RRg3D
        rDdM/sgakykrxV4a3cFkGT03qtGbeDGJZov5ogVKk7JqaHnlw3gchdPJdBZO4jCKD8zCSGIHS/Ez
        EEKIXftvPOqU/8JSTEbHnZKdw5xheTokBFijmh1A56TzqD2MepCM9qwb27pWagB4Y1RCqFRfuPt2
        g3UfFCqV4Kcftx+5evoTF+9TxCl91ze3d/GXQb1O+rFqDWW1plNAA/y0v3xRTAjQWLZcx2ipSJQq
        k9RQXbL2eEFLCECbt3DTB+xW8Ktm+7iC5QpQSXTsRGasaMK/Gs9CX9u1CaP5dgV7OJPaB5fWD4PI
        LGe1Q/U6S9TadPbaMB8OyP40N2Xyypq1e0GFTGrpisQyujaO4VSCo5HWAtRng4fKmrLyiTdbbote
        RbNOFfqL2aPT6wPojUc1YMXx6IJekrJH2d6M02UkpILTntpfSqxTaQZAMOj9tZtL2l3/Uuf/I98D
        RFx5TpPKcirpvOP+mOXm3f7r2Cnl1jA4tr8lceIl22YeKWdYq+5FgXt0nsskkzpnW1nZPivIqoSI
        5kzX0yiFYB88AwAA//8DAGN5VJJgBAAA
    headers:
      CF-RAY:
      - 9445c0450f6de801-ORD
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Fri, 23 May 2025 15:57:48 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=a0_Gq8.TtXD5YhXgjxMKAjqFFtbfdvAroZaCWvCj0sk-1748015868-1.0.1.1-sWocS0UNsdoMT4g1qURf8dcKR_inG6D64.m76fP2clwNI89Lyz3j.AVY3Drndat4_WjdjskbDWq4SixfrJJwzD5rkKVjFhHpKg55VNxoEmw;
        path=/; expires=Fri, 23-May-25 16:27:48 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=Uvh2PjIRamEjtrdSrr2HxW4fC5UkJQYvul9O.ZaXAbE-1748015868858-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      access-control-expose-headers:
      - X-Request-ID
      alt-svc:
      - h3=":443"; ma=86400
      cf-cache-status:
      - DYNAMIC
      openai-organization:
      - genomoncology-2v7ur8
      openai-processing-ms:
      - '547'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=31536000; includeSubDomains; preload
      x-envoy-upstream-service-time:
      - '549'
      x-ratelimit-limit-requests:
      - '5000'
      x-ratelimit-limit-tokens:
      - '800000'
      x-ratelimit-remaining-requests:
      - '4999'
      x-ratelimit-remaining-tokens:
      - '799955'
      x-ratelimit-reset-requests:
      - 12ms
      x-ratelimit-reset-tokens:
      - 3ms
      x-request-id:
      - req_f48eab541c83458078c6fed8b0aa3648
    status:
      code: 200
      message: OK
- request:
    body: '{"method":"initialize","params":{"protocolVersion":"2025-03-26","capabilities":{"sampling":{},"roots":{"listChanged":true}},"clientInfo":{"name":"mcp","version":"0.1.0"}},"jsonrpc":"2.0","id":0}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '194'
      content-type:
      - application/json
      host:
      - gitmcp.io
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: 'event: message

        data: {"jsonrpc":"2.0","id":0,"result":{"protocolVersion":"2025-03-26","capabilities":{"tools":{"listChanged":true}},"serverInfo":{"name":"GitMCP","version":"1.1.0"}}}


        '
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9445c04cd892eafc-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 15:57:49 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - f826043a8651fa66bd27a86f2da1a01343e569ebad34ec8b280f92ad6929de97
    status:
      code: 200
      message: OK
- request:
    body: '{"method":"notifications/initialized","jsonrpc":"2.0"}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '54'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - f826043a8651fa66bd27a86f2da1a01343e569ebad34ec8b280f92ad6929de97
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: ''
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9445c0913c0feafc-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 23 May 2025 15:57:59 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 202
      message: Accepted
- request:
    body: '{"method":"tools/call","params":{"name":"search_llm_documentation","arguments":{"query":"aliases
      for gpt-3.5-turbo-16k"}},"jsonrpc":"2.0","id":1}'
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '145'
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - f826043a8651fa66bd27a86f2da1a01343e569ebad34ec8b280f92ad6929de97
      user-agent:
      - python-httpx/0.28.1
    method: POST
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: "event: message\ndata: {\"jsonrpc\":\"2.0\",\"id\":1,\"result\":{\"searchQuery\":\"aliases
        for gpt-3.5-turbo-16k\",\"content\":[{\"type\":\"text\",\"text\":\"## Query\\n\\naliases
        for gpt-3.5-turbo-16k.\\n\\n## Response\\n\\n### Sources:\\nImportant: you
        can fetch the full content of any source using the fetch_url_content tool\\n\\n####
        (simonw/llm/llms.txt)[https://raw.githubusercontent.com/simonw/llm/main/llms.txt]
        (Score: 0.56)\\n- URLs as input to the model.\\n- **System Prompts**: Set
        a system prompt to influence the model's responses.\\n\\nKey technical terms
        and topics extracted from the document:\\n- LLM command line tool\\n- Large
        language models (LLMs)\\n- Model options and configuration\\n- Attachments
        and multi-modal input\\n- System prompts and conversation management\\n- Default
        options for models\\n- Prompt templates\\n- Tool execution and external functions\\n-
        Logging and conversation history\\n\\n\\n\\n#\\n- t.upper()\\n\\nmodel = llm.get_model(\\\"gpt-4.1-mini\\\")\\nresponse
        = model.prompt(\\\"Convert panda to upper\\\", tools=[upper])\\ntool_calls
        = response.tool_calls()\\n```\\n\\nKey technical terms and topics include:\\n\\n-
        **LLM**: Large Language Model\\n- **Model**: A specific LLM instance, e.g.,
        GPT-4o-mini\\n- **Prompt**: The input text to the model\\n- **Response**:
        The output from the model\\n- **Tools**: Functions that can be executed by
        the model\\n- **Attachments**: Files or URLs that can be passed to the model\\n-
        **Schemas**: JSON schema definitions for the response\\n- **Conversations**:
        Ongoing discussions with the model\\n\\nThe API supports various features,
        including:\\n\\n* **Lazy Loading**: Responses are not evaluated until the
        `text()` method is called\\n* **Async Models**: Support for asynchronous models
        using `llm.get_async_model()`\\n* **Streaming Responses**: Support for streaming
        responses as they are generated\\n* **Token Usage**: Methods for tracking
        token usage, e.g., `response.usage()` \\n\\nThe document also lists several
        functions for managing models, aliases, and defaults:\\n\\n* `llm.set_alias(alias,
        model_id)`: Define a new alias\\n* `llm.remove_alias(alias)`: Remove an alias\\n*
        `llm.set_default_model(model_id)`: Set the default model\\n* `llm.get_default_model()`:
        Get the currently configured default model\\n\\n\\n- [related-tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/related-tools.md):
        Related Tools for Working with LLMs\\nThis document describes tools designed
        to be used with Large Language Models (LLMs). It covers three main tools:
        `strip-tags`, `ttok`, and `symbex`. `strip-tags` is a command for stripping
        HTML tags, useful for working within token budget limits. `ttok` counts OpenAI
        tokens and can truncate input to a desired number of tokens. `symbex` searches
        for symbols in Python codebases, allowing for code extraction and piping into
        LLMs for various tasks. These tools enhance the functionality and usability
        of LLMs in different applications.\\n\\nExample usage of `strip-tags` and
        `ttok`:\\n```bash\\ncurl -s https://www.nytimes.com/ \\\\\\n\\n\\n#### (simonw/llm/docs/openai-models.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/openai-models.md]
        (Score: 0.55)\\n- \\n## OpenAI completion models\\n\\nThe `gpt-3.5-turbo-instruct`
        model is a little different - it is a completion model rather than a chat
        model, described in [the OpenAI completions documentation](https://platform.openai.com/docs/api-reference/completions/create).\\n\\nCompletion
        models can be called with the `-o logprobs 3` option (not supported by chat
        models) which will cause LLM to store 3 log probabilities for each returned
        token in the SQLite database. Consult [this issue](https://github.com/simonw/llm/issues/284#issuecomment-1724772704)
        for details on how to read these values.\\n\\n(openai-extra-models)=\\n\\n\\n-
        \\n## OpenAI language models\\n\\nRun `llm models` for a full list of available
        models. The OpenAI models supported by LLM are:\\n\\n<!-- [[[cog\\nfrom click.testing
        import CliRunner\\nfrom llm.cli import cli\\nresult = CliRunner().invoke(cli,
        [\\\"models\\\", \\\"list\\\"])\\nmodels = [line for line in result.output.split(\\\"\\\\n\\\")
        if line.startswith(\\\"OpenAI \\\")]\\ncog.out(\\\"```\\\\n{}\\\\n```\\\".format(\\\"\\\\n\\\".join(models)))\\n]]]
        -->\\n```\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOpenAI Chat: chatgpt-4o-latest
        (aliases: chatgpt-4o)\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOpenAI
        Chat: gpt-4o-audio-preview\\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\\nOpenAI
        Chat: gpt-4o-audio-preview-2024-10-01\\nOpenAI Chat: gpt-4o-mini-audio-preview\\nOpenAI
        Chat: gpt-4o-mini-audio-preview-2024-12-17\\nOpenAI Chat: gpt-4.1 (aliases:
        4.1)\\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\\nOpenAI Chat: gpt-4.1-nano
        (aliases: 4.1-nano)\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOpenAI
        Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOpenAI Chat: gpt-4
        (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOpenAI Chat:
        gpt-4-1106-preview\\nOpenAI Chat: gpt-4-0125-preview\\nOpenAI Chat: gpt-4-turbo-2024-04-09\\nOpenAI
        Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\\nOpenAI Chat:
        gpt-4.5-preview-2025-02-27\\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\\nOpenAI
        Chat: o1\\nOpenAI Chat: o1-2024-12-17\\nOpenAI Chat: o1-preview\\nOpenAI Chat:
        o1-mini\\nOpenAI Chat: o3-mini\\nOpenAI Chat: o3\\nOpenAI Chat: o4-mini\\nOpenAI
        Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\\n```\\n<!--
        [[[end]]] -->\\n\\nSee [the OpenAI models documentation](https://platform.openai.com/docs/models)
        for details of each of these.\\n\\n`gpt-4o-mini` (aliased to `4o-mini`) is
        the least expensive model, and is the default for if you don't specify a model
        at all. Consult [OpenAI's model documentation](https://platform.openai.com/docs/models)
        for details of the other models.\\n\\n\\n#### (simonw/llm/docs/tools.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md]
        (Score: 0.54)\\n- \\n\\nMany Large Language Models have been trained to execute
        tools as part of responding to a prompt. LLM supports tool usage with both
        the command-line interface and the Python API.\\n\\n#\\n\\n#### (simonw/llm/docs/aliases.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/aliases.md]
        (Score: 0.54)\\n- \\n# Model aliases\\n\\nLLM supports model aliases, which
        allow you to refer to a model by a short name instead of its full ID.\\n\\n##
        Listing aliases\\n\\nTo list current aliases, run this:\\n\\n```bash\\nllm
        aliases\\n```\\nExample output:\\n\\n<!-- [[[cog\\nfrom click.testing import
        CliRunner\\nfrom llm.cli import cli\\nresult = CliRunner().invoke(cli, [\\\"aliases\\\",
        \\\"list\\\"])\\ncog.out(\\\"```\\\\n{}```\\\".format(result.output))\\n]]]
        -->\\n```\\n4o                  : gpt-4o\\nchatgpt-4o          : chatgpt-4o-latest\\n4o-mini
        \            : gpt-4o-mini\\n4.1                 : gpt-4.1\\n4.1-mini            :
        gpt-4.1-mini\\n4.1-nano            : gpt-4.1-nano\\n3.5                 :
        gpt-3.5-turbo\\nchatgpt             : gpt-3.5-turbo\\nchatgpt-16k         :
        gpt-3.5-turbo-16k\\n3.5-16k             : gpt-3.5-turbo-16k\\n4                   :
        gpt-4\\ngpt4                : gpt-4\\n4-32k               : gpt-4-32k\\ngpt-4-turbo-preview
        : gpt-4-turbo\\n4-turbo             : gpt-4-turbo\\n4t                  :
        gpt-4-turbo\\ngpt-4.5             : gpt-4.5-preview\\n3.5-instruct        :
        gpt-3.5-turbo-instruct\\nchatgpt-instruct    : gpt-3.5-turbo-instruct\\nada
        \                : text-embedding-ada-002 (embedding)\\nada-002             :
        text-embedding-ada-002 (embedding)\\n3-small             : text-embedding-3-small
        (embedding)\\n3-large             : text-embedding-3-large (embedding)\\n3-small-512
        \        : text-embedding-3-small-512 (embedding)\\n3-large-256         :
        text-embedding-3-large-256 (embedding)\\n3-large-1024        : text-embedding-3-large-1024
        (embedding)\\n```\\n<!-- [[[end]]] -->\\n\\nAdd `--json` to get that list
        back as JSON:\\n\\n```bash\\nllm aliases list --json\\n```\\nExample output:\\n```json\\n{\\n
        \   \\\"3.5\\\": \\\"gpt-3.5-turbo\\\",\\n    \\\"chatgpt\\\": \\\"gpt-3.5-turbo\\\",\\n
        \   \\\"4\\\": \\\"gpt-4\\\",\\n    \\\"gpt4\\\": \\\"gpt-4\\\",\\n    \\\"ada\\\":
        \\\"ada-002\\\"\\n}\\n```\\n\\n\\n\\n#### (simonw/llm/docs/usage.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md]
        (Score: 0.54)\\n- \\n\\nSome models are completion models - rather than being
        tuned to respond to chat style prompts, they are designed to complete a sentence
        or paragraph.\\n\\nAn example of this is the `gpt-3.5-turbo-instruct` OpenAI
        model.\\n\\nYou can prompt that model the same way as the chat models, but
        be aware that the prompt format that works best is likely to differ.\\n\\n```bash\\nllm
        -m gpt-3.5-turbo-instruct 'Reasons to tame a wild beaver:'\\n```\\n\\n(usage-chat)=\\n\\n\\n\\n####
        (simonw/llm/docs/schemas.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/schemas.md]
        (Score: 0.54)\\n- \\n\\nLarge Language Models are very good at producing structured
        output as JSON or other formats. LLM's **schemas** feature allows you to define
        the exact structure of JSON data you want to receive from a model.\\n\\nThis
        feature is supported by models from OpenAI, Anthropic, Google Gemini and can
        be implemented for others {ref}`via plugins <advanced-model-plugins-schemas>`.\\n\\nThis
        page describes schemas used via the `llm` command-line tool. Schemas can also
        be used from the {ref}`Python API <python-api-schemas>`.\\n\\n(schemas-tutorial)=\\n\\n#\\n\\n####
        (simonw/llm/docs/setup.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/setup.md]
        (Score: 0.52)\\n- \\n\\nMany LLM models require an API key. These API keys
        can be provided to this tool using several different mechanisms.\\n\\nYou
        can obtain an API key for OpenAI's language models from [the API keys page](https://platform.openai.com/api-keys)
        on their site.\\n\\n#\\n\\n#### (simonw/llm/README.md)[https://raw.githubusercontent.com/simonw/llm/main/README.md]
        (Score: 0.52)\\n- \\n\\nFull documentation: **[llm.datasette.io](https://llm.datasette.io/)**\\n\\nBackground
        on this project:\\n\\n- [llm, ttok and strip-tags\u2014CLI tools for working
        with ChatGPT and other LLMs](https://simonwillison.net/2023/May/18/cli-tools-for-llms/)\\n-
        [The LLM CLI tool now supports self-hosted language models via plugins](https://simonwillison.net/2023/Jul/12/llm/)\\n-
        [LLM now provides tools for working with embeddings](https://simonwillison.net/2023/Sep/4/llm-embeddings/)\\n-
        [Build an image search engine with llm-clip, chat with models with llm chat](https://simonwillison.net/2023/Sep/12/llm-clip-and-chat/)\\n-
        [You can now run prompts against images, audio and video in your terminal
        using LLM](https://simonwillison.net/2024/Oct/29/llm-multi-modal/)\\n- [Structured
        data extraction from unstructured content using LLM schemas](https://simonwillison.net/2025/Feb/28/llm-schemas/)\\n-
        [Long context support in LLM 0.24 using fragments and template plugins](https://simonwillison.net/2025/Apr/7/long-context-llm/)\\n\"}]}}\n\n"
    headers:
      Access-Control-Allow-Headers:
      - Content-Type, mcp-session-id
      Access-Control-Allow-Methods:
      - GET, POST, OPTIONS
      Access-Control-Allow-Origin:
      - '*'
      Access-Control-Expose-Headers:
      - mcp-session-id
      Access-Control-Max-Age:
      - '86400'
      CF-RAY:
      - 9445c0917ccfeafc-ORD
      Cache-Control:
      - no-cache
      Connection:
      - keep-alive
      Content-Type:
      - text/event-stream
      Date:
      - Fri, 23 May 2025 15:57:59 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
      mcp-session-id:
      - f826043a8651fa66bd27a86f2da1a01343e569ebad34ec8b280f92ad6929de97
    status:
      code: 200
      message: OK
- request:
    body: ''
    headers:
      accept:
      - application/json, text/event-stream
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-type:
      - application/json
      host:
      - gitmcp.io
      mcp-session-id:
      - f826043a8651fa66bd27a86f2da1a01343e569ebad34ec8b280f92ad6929de97
      user-agent:
      - python-httpx/0.28.1
    method: DELETE
    uri: https://gitmcp.io/simonw/llm
  response:
    body:
      string: '{"jsonrpc":"2.0","error":{"code":-32000,"message":"Method not allowed"},"id":null}'
    headers:
      CF-RAY:
      - 9445c0ee0b87eafc-ORD
      Connection:
      - keep-alive
      Content-Length:
      - '82'
      Content-Type:
      - text/plain;charset=UTF-8
      Date:
      - Fri, 23 May 2025 15:58:14 GMT
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 405
      message: Method Not Allowed
- request:
    body: "{\"messages\":[{\"role\":\"user\",\"content\":\"According to the `llm`,
      what are the aliases for the model named 'gpt-3.5-turbo-16k'? Respond **only**
      with JSON formatted as {\\\"aliases\\\": [\\\"alias1\\\", \\\"alias2\\\", ...]}.\"},{\"role\":\"assistant\",\"tool_calls\":[{\"type\":\"function\",\"id\":\"call_aIYKHepzw4hCdaa2cTnFKS4O\",\"function\":{\"name\":\"search_llm_documentation\",\"arguments\":\"{\\\"query\\\":
      \\\"aliases for gpt-3.5-turbo-16k\\\"}\"}}]},{\"role\":\"tool\",\"tool_call_id\":\"call_aIYKHepzw4hCdaa2cTnFKS4O\",\"content\":\"##
      Query\\n\\naliases for gpt-3.5-turbo-16k.\\n\\n## Response\\n\\n### Sources:\\nImportant:
      you can fetch the full content of any source using the fetch_url_content tool\\n\\n####
      (simonw/llm/llms.txt)[https://raw.githubusercontent.com/simonw/llm/main/llms.txt]
      (Score: 0.56)\\n- URLs as input to the model.\\n- **System Prompts**: Set a
      system prompt to influence the model's responses.\\n\\nKey technical terms and
      topics extracted from the document:\\n- LLM command line tool\\n- Large language
      models (LLMs)\\n- Model options and configuration\\n- Attachments and multi-modal
      input\\n- System prompts and conversation management\\n- Default options for
      models\\n- Prompt templates\\n- Tool execution and external functions\\n- Logging
      and conversation history\\n\\n\\n\\n#\\n- t.upper()\\n\\nmodel = llm.get_model(\\\"gpt-4.1-mini\\\")\\nresponse
      = model.prompt(\\\"Convert panda to upper\\\", tools=[upper])\\ntool_calls =
      response.tool_calls()\\n```\\n\\nKey technical terms and topics include:\\n\\n-
      **LLM**: Large Language Model\\n- **Model**: A specific LLM instance, e.g.,
      GPT-4o-mini\\n- **Prompt**: The input text to the model\\n- **Response**: The
      output from the model\\n- **Tools**: Functions that can be executed by the model\\n-
      **Attachments**: Files or URLs that can be passed to the model\\n- **Schemas**:
      JSON schema definitions for the response\\n- **Conversations**: Ongoing discussions
      with the model\\n\\nThe API supports various features, including:\\n\\n* **Lazy
      Loading**: Responses are not evaluated until the `text()` method is called\\n*
      **Async Models**: Support for asynchronous models using `llm.get_async_model()`\\n*
      **Streaming Responses**: Support for streaming responses as they are generated\\n*
      **Token Usage**: Methods for tracking token usage, e.g., `response.usage()`
      \\n\\nThe document also lists several functions for managing models, aliases,
      and defaults:\\n\\n* `llm.set_alias(alias, model_id)`: Define a new alias\\n*
      `llm.remove_alias(alias)`: Remove an alias\\n* `llm.set_default_model(model_id)`:
      Set the default model\\n* `llm.get_default_model()`: Get the currently configured
      default model\\n\\n\\n- [related-tools.md](https://raw.githubusercontent.com/simonw/llm/main/docs/related-tools.md):
      Related Tools for Working with LLMs\\nThis document describes tools designed
      to be used with Large Language Models (LLMs). It covers three main tools: `strip-tags`,
      `ttok`, and `symbex`. `strip-tags` is a command for stripping HTML tags, useful
      for working within token budget limits. `ttok` counts OpenAI tokens and can
      truncate input to a desired number of tokens. `symbex` searches for symbols
      in Python codebases, allowing for code extraction and piping into LLMs for various
      tasks. These tools enhance the functionality and usability of LLMs in different
      applications.\\n\\nExample usage of `strip-tags` and `ttok`:\\n```bash\\ncurl
      -s https://www.nytimes.com/ \\\\\\n\\n\\n#### (simonw/llm/docs/openai-models.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/openai-models.md]
      (Score: 0.55)\\n- \\n## OpenAI completion models\\n\\nThe `gpt-3.5-turbo-instruct`
      model is a little different - it is a completion model rather than a chat model,
      described in [the OpenAI completions documentation](https://platform.openai.com/docs/api-reference/completions/create).\\n\\nCompletion
      models can be called with the `-o logprobs 3` option (not supported by chat
      models) which will cause LLM to store 3 log probabilities for each returned
      token in the SQLite database. Consult [this issue](https://github.com/simonw/llm/issues/284#issuecomment-1724772704)
      for details on how to read these values.\\n\\n(openai-extra-models)=\\n\\n\\n-
      \\n## OpenAI language models\\n\\nRun `llm models` for a full list of available
      models. The OpenAI models supported by LLM are:\\n\\n<!-- [[[cog\\nfrom click.testing
      import CliRunner\\nfrom llm.cli import cli\\nresult = CliRunner().invoke(cli,
      [\\\"models\\\", \\\"list\\\"])\\nmodels = [line for line in result.output.split(\\\"\\\\n\\\")
      if line.startswith(\\\"OpenAI \\\")]\\ncog.out(\\\"```\\\\n{}\\\\n```\\\".format(\\\"\\\\n\\\".join(models)))\\n]]]
      -->\\n```\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOpenAI Chat: chatgpt-4o-latest
      (aliases: chatgpt-4o)\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOpenAI
      Chat: gpt-4o-audio-preview\\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\\nOpenAI
      Chat: gpt-4o-audio-preview-2024-10-01\\nOpenAI Chat: gpt-4o-mini-audio-preview\\nOpenAI
      Chat: gpt-4o-mini-audio-preview-2024-12-17\\nOpenAI Chat: gpt-4.1 (aliases:
      4.1)\\nOpenAI Chat: gpt-4.1-mini (aliases: 4.1-mini)\\nOpenAI Chat: gpt-4.1-nano
      (aliases: 4.1-nano)\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOpenAI
      Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOpenAI Chat: gpt-4
      (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOpenAI Chat:
      gpt-4-1106-preview\\nOpenAI Chat: gpt-4-0125-preview\\nOpenAI Chat: gpt-4-turbo-2024-04-09\\nOpenAI
      Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\\nOpenAI Chat:
      gpt-4.5-preview-2025-02-27\\nOpenAI Chat: gpt-4.5-preview (aliases: gpt-4.5)\\nOpenAI
      Chat: o1\\nOpenAI Chat: o1-2024-12-17\\nOpenAI Chat: o1-preview\\nOpenAI Chat:
      o1-mini\\nOpenAI Chat: o3-mini\\nOpenAI Chat: o3\\nOpenAI Chat: o4-mini\\nOpenAI
      Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\\n```\\n<!--
      [[[end]]] -->\\n\\nSee [the OpenAI models documentation](https://platform.openai.com/docs/models)
      for details of each of these.\\n\\n`gpt-4o-mini` (aliased to `4o-mini`) is the
      least expensive model, and is the default for if you don't specify a model at
      all. Consult [OpenAI's model documentation](https://platform.openai.com/docs/models)
      for details of the other models.\\n\\n\\n#### (simonw/llm/docs/tools.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/tools.md]
      (Score: 0.54)\\n- \\n\\nMany Large Language Models have been trained to execute
      tools as part of responding to a prompt. LLM supports tool usage with both the
      command-line interface and the Python API.\\n\\n#\\n\\n#### (simonw/llm/docs/aliases.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/aliases.md]
      (Score: 0.54)\\n- \\n# Model aliases\\n\\nLLM supports model aliases, which
      allow you to refer to a model by a short name instead of its full ID.\\n\\n##
      Listing aliases\\n\\nTo list current aliases, run this:\\n\\n```bash\\nllm aliases\\n```\\nExample
      output:\\n\\n<!-- [[[cog\\nfrom click.testing import CliRunner\\nfrom llm.cli
      import cli\\nresult = CliRunner().invoke(cli, [\\\"aliases\\\", \\\"list\\\"])\\ncog.out(\\\"```\\\\n{}```\\\".format(result.output))\\n]]]
      -->\\n```\\n4o                  : gpt-4o\\nchatgpt-4o          : chatgpt-4o-latest\\n4o-mini
      \            : gpt-4o-mini\\n4.1                 : gpt-4.1\\n4.1-mini            :
      gpt-4.1-mini\\n4.1-nano            : gpt-4.1-nano\\n3.5                 : gpt-3.5-turbo\\nchatgpt
      \            : gpt-3.5-turbo\\nchatgpt-16k         : gpt-3.5-turbo-16k\\n3.5-16k
      \            : gpt-3.5-turbo-16k\\n4                   : gpt-4\\ngpt4                :
      gpt-4\\n4-32k               : gpt-4-32k\\ngpt-4-turbo-preview : gpt-4-turbo\\n4-turbo
      \            : gpt-4-turbo\\n4t                  : gpt-4-turbo\\ngpt-4.5             :
      gpt-4.5-preview\\n3.5-instruct        : gpt-3.5-turbo-instruct\\nchatgpt-instruct
      \   : gpt-3.5-turbo-instruct\\nada                 : text-embedding-ada-002
      (embedding)\\nada-002             : text-embedding-ada-002 (embedding)\\n3-small
      \            : text-embedding-3-small (embedding)\\n3-large             : text-embedding-3-large
      (embedding)\\n3-small-512         : text-embedding-3-small-512 (embedding)\\n3-large-256
      \        : text-embedding-3-large-256 (embedding)\\n3-large-1024        : text-embedding-3-large-1024
      (embedding)\\n```\\n<!-- [[[end]]] -->\\n\\nAdd `--json` to get that list back
      as JSON:\\n\\n```bash\\nllm aliases list --json\\n```\\nExample output:\\n```json\\n{\\n
      \   \\\"3.5\\\": \\\"gpt-3.5-turbo\\\",\\n    \\\"chatgpt\\\": \\\"gpt-3.5-turbo\\\",\\n
      \   \\\"4\\\": \\\"gpt-4\\\",\\n    \\\"gpt4\\\": \\\"gpt-4\\\",\\n    \\\"ada\\\":
      \\\"ada-002\\\"\\n}\\n```\\n\\n\\n\\n#### (simonw/llm/docs/usage.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/usage.md]
      (Score: 0.54)\\n- \\n\\nSome models are completion models - rather than being
      tuned to respond to chat style prompts, they are designed to complete a sentence
      or paragraph.\\n\\nAn example of this is the `gpt-3.5-turbo-instruct` OpenAI
      model.\\n\\nYou can prompt that model the same way as the chat models, but be
      aware that the prompt format that works best is likely to differ.\\n\\n```bash\\nllm
      -m gpt-3.5-turbo-instruct 'Reasons to tame a wild beaver:'\\n```\\n\\n(usage-chat)=\\n\\n\\n\\n####
      (simonw/llm/docs/schemas.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/schemas.md]
      (Score: 0.54)\\n- \\n\\nLarge Language Models are very good at producing structured
      output as JSON or other formats. LLM's **schemas** feature allows you to define
      the exact structure of JSON data you want to receive from a model.\\n\\nThis
      feature is supported by models from OpenAI, Anthropic, Google Gemini and can
      be implemented for others {ref}`via plugins <advanced-model-plugins-schemas>`.\\n\\nThis
      page describes schemas used via the `llm` command-line tool. Schemas can also
      be used from the {ref}`Python API <python-api-schemas>`.\\n\\n(schemas-tutorial)=\\n\\n#\\n\\n####
      (simonw/llm/docs/setup.md)[https://raw.githubusercontent.com/simonw/llm/main/docs/setup.md]
      (Score: 0.52)\\n- \\n\\nMany LLM models require an API key. These API keys can
      be provided to this tool using several different mechanisms.\\n\\nYou can obtain
      an API key for OpenAI's language models from [the API keys page](https://platform.openai.com/api-keys)
      on their site.\\n\\n#\\n\\n#### (simonw/llm/README.md)[https://raw.githubusercontent.com/simonw/llm/main/README.md]
      (Score: 0.52)\\n- \\n\\nFull documentation: **[llm.datasette.io](https://llm.datasette.io/)**\\n\\nBackground
      on this project:\\n\\n- [llm, ttok and strip-tags\u2014CLI tools for working
      with ChatGPT and other LLMs](https://simonwillison.net/2023/May/18/cli-tools-for-llms/)\\n-
      [The LLM CLI tool now supports self-hosted language models via plugins](https://simonwillison.net/2023/Jul/12/llm/)\\n-
      [LLM now provides tools for working with embeddings](https://simonwillison.net/2023/Sep/4/llm-embeddings/)\\n-
      [Build an image search engine with llm-clip, chat with models with llm chat](https://simonwillison.net/2023/Sep/12/llm-clip-and-chat/)\\n-
      [You can now run prompts against images, audio and video in your terminal using
      LLM](https://simonwillison.net/2024/Oct/29/llm-multi-modal/)\\n- [Structured
      data extraction from unstructured content using LLM schemas](https://simonwillison.net/2025/Feb/28/llm-schemas/)\\n-
      [Long context support in LLM 0.24 using fragments and template plugins](https://simonwillison.net/2025/Apr/7/long-context-llm/)\\n\"}],\"model\":\"gpt-4.1\",\"stream\":false,\"tools\":[{\"type\":\"function\",\"function\":{\"name\":\"fetch_llm_documentation\",\"description\":\"Fetch
      entire documentation file from GitHub repository: simonw/llm. Useful for general
      questions. Always call this tool first if asked about simonw/llm.\",\"parameters\":{}}},{\"type\":\"function\",\"function\":{\"name\":\"search_llm_documentation\",\"description\":\"Semantically
      search within the fetched documentation from GitHub repository: simonw/llm.
      Useful for specific queries.\",\"parameters\":{\"type\":\"object\",\"properties\":{\"query\":{\"type\":\"string\",\"description\":\"The
      search query to find relevant documentation\"}},\"required\":[\"query\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}}},{\"type\":\"function\",\"function\":{\"name\":\"search_llm_code\",\"description\":\"Search
      for code within the GitHub repository: \\\"simonw/llm\\\" using the GitHub Search
      API (exact match). Returns matching files for you to query further if relevant.\",\"parameters\":{\"type\":\"object\",\"properties\":{\"query\":{\"type\":\"string\",\"description\":\"The
      search query to find relevant code files\"},\"page\":{\"type\":\"number\",\"description\":\"Page
      number to retrieve (starting from 1). Each page contains 30 results.\"}},\"required\":[\"query\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}}},{\"type\":\"function\",\"function\":{\"name\":\"fetch_generic_url_content\",\"description\":\"Generic
      tool to fetch content from any absolute URL, respecting robots.txt rules. Use
      this to retrieve referenced urls (absolute urls) that were mentioned in previously
      fetched documentation.\",\"parameters\":{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\",\"description\":\"The
      URL of the document or page to fetch\"}},\"required\":[\"url\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}}}]}"
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '12913'
      content-type:
      - application/json
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.79.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.79.0
      x-stainless-read-timeout:
      - '600'
      x-stainless-retry-count:
      - '0'
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.12.7
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAAwAAAP//jFLBbuMgEL37K9Cc7chO7DTJcXup9lB1N+ppHVkUxg5bDAhwtVWUf1+B
        09jdbaVekODNe7w3M6eEEBAcdgTYkXrWG5l9ow9ll69v8Ud1393x/OFlv380P/e338X9HaSBoZ9+
        I/NvrAXTvZHohVYjzCxSj0G1uCk3eVFttmUEes1RBlpnfFYuimyZL6ssL7OivDCPWjB0sCO/EkII
        OcUzeFQc/8CO5OnbS4/O0Q5hdy0iBKyW4QWoc8J5qjykE8i08qii7VMNVArq0NXhrzrmCKaK9XMN
        KalhtajGy+E817DYDo6GDGqQcgZQpbSnoQfR/eGCnK9+pe6M1U/uHyq0Qgl3bCxSp1Xw5rw2ENFz
        Qsgh9mV4FxWM1b3xjdfPGL9brarNKAjTLCZ4WVxArz2Vc9rNNv1AseHoqZBu1ltglB2RT9xpEHTg
        Qs+AZJb7fzsfaY/Zheq+Ij8BjKHxyBtjkQv2PvJUZjHs6mdl1z5Hw+DQvgiGjRdowyw4tnSQ4xaB
        e3Ue+6YVqkNrrBhXqTUNY2yNbLssOCTn5C8AAAD//wMAcf6CaFQDAAA=
    headers:
      CF-RAY:
      - 9445c0eeaa2ee7fa-ORD
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Fri, 23 May 2025 15:58:15 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=0HMh3XW2zQZLEbNOcYxdgYgXo8w4V0OxiMjNrKogmMw-1748015895-1.0.1.1-3JVqlxxkcpKEi1kGizCVV2JVUZHlKs8HlyOwp6JDORJB6us4FsgIhN.xOFPejUbInDQO2zhkWU7HEf_8rOfPHBsA5hUNq5rbNxuNIfPjW7s;
        path=/; expires=Fri, 23-May-25 16:28:15 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=6xeOpjwWZbx8HcAlMzawQZ1vsEJRmEXVKoQ2nQzOtz4-1748015895607-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      access-control-expose-headers:
      - X-Request-ID
      alt-svc:
      - h3=":443"; ma=86400
      cf-cache-status:
      - DYNAMIC
      openai-organization:
      - genomoncology-2v7ur8
      openai-processing-ms:
      - '702'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=31536000; includeSubDomains; preload
      x-envoy-upstream-service-time:
      - '705'
      x-ratelimit-limit-requests:
      - '5000'
      x-ratelimit-limit-tokens:
      - '800000'
      x-ratelimit-remaining-requests:
      - '4999'
      x-ratelimit-remaining-tokens:
      - '797381'
      x-ratelimit-reset-requests:
      - 12ms
      x-ratelimit-reset-tokens:
      - 196ms
      x-request-id:
      - req_fca6d44bf1edd29d8e416ae007bd0814
    status:
      code: 200
      message: OK
version: 1
